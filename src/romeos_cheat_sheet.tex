\noindent Summary for \emph{Computational Statistics 2021 @ ETH}\\
by Romeo Valentin (\url{github.com/RomeoV})
\section{Multiple Linear Regression}\label{sec:multiple_linear_regression}
\subsection{The Linear Model}\label{subsec:the_linear_model}
\begin{sectionbox}\nospacing{}
Assume $Y_i = x_{i}^{\top}\beta + \epsilon_{i}$ or $Y = X \times \beta + \epsilon$ with $X \in \mathcal{R}^{(n \times p)}; (n > p)$ and $\mathbb{E}[\epsilon_{i}]=0, Var(\epsilon_{i}) = \sigma^{2}$.
$X$ is often augmented with $(1_{N\times 1})$ to use $\beta_{1}$ as bias.
\end{sectionbox}
\subsection{Least Squares Method}\label{subsec:least_squares_method}
\begin{sectionbox}\nospacing{}
  Least squares estimator is $\hat \beta = \argmin_{\beta} ||Y - X\beta||^{2}_{2} = {(X^{\top}X)}^{-1} X^{\top}Y$ (orthogonal projection of $Y$ onto $span(X)$).
  Estimate $\hat{\sigma}^{2} = \frac{1}{n-p}\sum_{i=1}^{n}r_{i}^{2}$ with $\mathbb{E}[\hat{\sigma}^{2}] = \sigma^{2}$.
\end{sectionbox}

\begin{notebox}[Assumptions for Linear Model]\nospacing{}
    \begin{enumeratenosep}[label=\roman*]
        \item Linear regression equation is correct, i.e. $\mathbb{E}[\epsilon_{i}]=0\ \forall i$.
        \item We measure $x_{i}$'s exactly. Else, need correction (?).
        \item Error is homoscedastic, i.e. $Var(\epsilon_{i})=\sigma^{2}\ \forall i$. Else, use ``Weighted LS''.
        \item Errors are uncorrelated, i.e. $Cov(\epsilon_{i}, \epsilon_{j}) = 0\ \forall i \neq j$. Else ``Generalized LS''.
        \item Errors are jointly normally distributed. Else ``Robust Methods''.
    \end{enumeratenosep}
\end{notebox}

\begin{notebox}[Moments of least squares estimates]\nospacing{}
  Assume $\vec{Y}=X\vec{\beta} + \epsilon, \mathbb{E}[\vec{\epsilon}]=\vec{0},\ Cov(\vec{\epsilon} \vec{\epsilon}^{\top}) = \sigma^{2}I$ (all assumptions satisfied). Then
  \begin{enumeratenosep}[label=\roman*]
    \item $\mathbb{E}[\hat{\vec{\beta}}] = \vec{\beta}$ ($\hat{\vec{\beta}}$ is unbiased).
    \item $\mathbb{E}[\hat{\vec{Y}}] = \mathbb{E}[\vec{Y}] = X\vec{\beta}$ and $\mathbb{E}[\vec{r}] = \vec{0}$.
    \item $Cov(\hat{\vec{\beta}}) = \sigma^{2}{(X^{\top}X)}^{-1}$.
    \item $Cov(\hat{\vec{Y}}) = \sigma^{2} P, Cov(\vec{r}) = \sigma^{2}(I-P)$.
  \end{enumeratenosep}
  If additionally $\epsilon_{i}, \dots, \epsilon_{n} \text{ i.i.d. } \sim \mathcal{N}(0, \sigma^{2})$, then
  \begin{enumeratenosep}[label=\roman*]
    \item $\hat{\vec{\beta}} \sim \mathcal{N}_{p}{(\vec{\beta}, \sigma^{2}(X^{\top}X))}^{-1}$
    \item $\hat{\vec{Y}} \sim \mathcal{N}_{n}(X\vec{\beta}, \sigma^{2}),\ \vec{r} \sim \mathbb{N}_{n}(\vec{0}, \sigma^{2}(I-P))$
    \item $\hat \sigma^{2} \sim \frac{\sigma^{2}}{n-p}\chi^{2}_{n-p}$.
  \end{enumeratenosep}
  Even when normality assumption doesn't hold, central limit theorem is a justification.
\end{notebox}
\subsection{Tests and Confidence Regions}\label{subsec:tests_and_confidence_regions}
\begin{sectionbox}[T-test]\nospacing{}
  Assume linear model with Gaussian errors (or ``large enough'' sample size), s.t. $\hat{\vec{\beta}} \sim \mathcal{N}_{p}\left(\vec{\beta}, \sigma^{2}{(X^{\top}X)}^{-1}\right)$ is normally distributed.
  Then we can test the null-hypothesis $H_{0,j}: \beta_{j = 0}$ against $H_{A,j}: \beta_{j \neq 0}$:
  \[\frac{\hat \beta_{j}}{\sqrt{\sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim \mathcal{N}(0,1)\] under the null-hypothesis $H_{0,j}$. Since $\sigma^{2}$ is unknown, replace by $\hat \sigma^{2} \rightarrow$ T-test:
  \[T_{j} = \frac{\hat \beta_{j}}{\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim t_{n-p}\] under the null-hypothesis $H_{0,j}$. Note that $t_{n-p} \approx \mathcal{N}$.
\end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 As individual t-test for $H_{0,j}$ gives the effect of $\beta_{j}$ after subtracting the linear effect of all $\beta_{i\neq j}$.
\end{attentionbox}

 \begin{sectionbox}[Global null hypothesis and ANOVA]\nospacing{}
  We can also check the global null-hypothesis $H_{0}: \beta_{2} = \cdots = \beta_{p} = 0$ using the \emph{analysis of variance} (ANOVA), which decomposes
  \[||\vec{Y} - \bar{\vec{Y}}||^{2}_{2} = ||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}_{2} + ||\vec{Y} - \hat{\vec{Y}}||^{2}_{2}.\]
  Under the global null-hypothesis $\mathbb{E}[\vec{Y}] = \mathcal{E}[\bar{\vec{Y}}] = const$. (no effect of predictor variables). Basically dividing $\sigma^{2}$ by $\hat \sigma^{2}$ yields F-statistic:
  \[F = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}/(p-1)}{||\vec{Y} - \hat{\vec{Y}}||^{2} / (n-p)} \sim F_{p-1,n-p}\] under the global null-hypothesis $H_{0}$.
  ANOVA also yields \emph{goodness of fit} $R^{2} = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}}{||\vec{Y} - \bar{\vec{Y}}||^{2}}$, which should be around $1$.
  Finally, we can also build a confidence interval using $\hat \beta_{j} \pm \sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} \cdot t_{n-p;1-\alpha/2}$.
 \end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 In \verb!summary.lm!, the term \emph{Std. Error} is $\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} = \sqrt{\hat{Var}(\hat \beta_{j})}$.
\end{attentionbox}
\todo[inline]{ANOVA function for determining if a model is worth it.}

\subsection{Checking Model Assumptions}\label{subsec:checking_model_assumptions}
\begin{sectionbox}[Tukey-Anscome Plot]\nospacing{}
  Error should fluctuate randomly. If error increases linearly, do log-transform $\vec{Y} \mapsto \log{\vec{Y}}$. If error increases with $\sqrt{Y}$, do a square-root-transform $\vec{Y} \mapsto \sqrt{\vec{Y}}$.
\end{sectionbox}
\begin{sectionbox}[QQ-Plot/Normal-Plot]\nospacing{}
  Plot empirical quantiles of residuals on y versus the theoretical quantiles of $\mathcal{N}(0,1)$ on x.
  If assumption holds, get straight line with intercept $\mu$ and slope $\sigma$.
  Z-shape: long-tailed distr.; Curved: skewed distr.
\end{sectionbox}

\subsection{Model Selection}\label{subsec:model_selection}
\begin{sectionbox}\nospacing{}
  Assume again $\mathbb{E}[\epsilon_{i}] = 0, Var(\epsilon_{i} = \sigma^{2})$.
  We need to address \emph{bias-variance trade-off}.
  Bias is defined as $\mathbb{E} [\hat f(x)] - f(x)$, variance as $q/n \cdot \sigma^{2}$ with $q \leq p$.
\end{sectionbox}
\begin{sectionbox}[Mallows $C_{p}$ statistic]\nospacing{}
  Let $SSE(\mathcal{M})$ the residual sum of squares.
  Then $n^{-1} \sum_{i=1}^{n} \mathbb{E}\left[{(f(x) - \hat f_{\mathcal{I}}(x))}^{2}\right] \approx n^{-1}SSE(\mathcal{M})-\hat \sigma^{2} + 2\hat\sigma^{2}|\mathcal{M}|/n$, with $\mathcal{I}$ the indices of selected predictors and $|\mathcal{I}| = q$.
  Thus, we search for the model that minimizes the $C_{p}$-statistic with $C_{p}(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat \sigma^{2}} - n + 2|\mathcal{M}|$.
  Otherwise Akaike's information criterion (AIC) or Bayesian information criterion (BIC). AIC is equivalent to $C_{p}$ for linear Gaussian models.
  \begin{mintlinebox}{R}
    require(leaps); fit.all <- regsubsets(y~., data=data)
    p.regsubsets(fit.all)
  \end{mintlinebox}
\end{sectionbox}
\begin{sectionbox}[Forwards and backwards selection]\nospacing{}
  \textbf{Forward selection:} (i) Start with empty model. (ii) (Greedily) Keep adding variable that reduces the residual sum of squares. (iii) When done, pick submodel which minimizes $C_{p}$.

  \textbf{Backward selection:} (i) Start with full model. (ii) (Greedily) Keep excluding predictor that increases the residual sum of squares the least. (iii) When done, pick submodel which minimizes $C_{p}$.
  Backwards selection typically better but more expensive. When $p \geq n$, use forward selection.
  Both methods prone to overfitting --- p-values (and similar values) are \emph{not} valid anymore and effects look too significant.
  \begin{mintlinebox}{R}
    fit.empty <- lm(y~1, data=data)
    fit.full <- lm(y~., data=data)
    fit.bw <- step(fit.full, direction="backward")
    fit.fw <- step(fit.full, direction="forward")
  \end{mintlinebox}

\end{sectionbox}

\section{Nonparametric Density Estimation}\label{sec:nonparametric_density_estimation}
\begin{sectionbox}[Kernel estimator]
  Estimate density $\hat f(x) = \frac{1}{nh}\sum_{i=1}^n w((x-X_i)/h)$.
  Kernels include (i) rectangular ($w(x) = 0.5\cdot\mathds{1}_{|x|<1}$), (ii) triangular, or (iii) Gaussian.
  We require $\int_\mathbb{R}K(x)dx = 1$.
  The bandwidth parameter $h$ is crucial and determines the ``smoothness'' of the density estimate.
\end{sectionbox}
\begin{sectionbox}[Choosing a bandwidth $h$]\nospacing{}
  A simple approach is using $k$-nearest neighbors, i.e. $h(x)=\max_{x_i \in \mathit{KNN}_k(x)} ||x-x_i||_2$ with tuning parameter $k$.
  Note that $\int_\mathbb{R} K(x)dx = 1$ might be violated.
  Naturally, the bandwidth also induces a \emph{bias-variance trade-off}.
  Note that
  \(
    MSE(x) = \mathbb{E}\left[\left(\hat f(x)-f(x)\right)^2\right]=\left(\mathbb{E}[\hat f(x)] - f(x)\right)^2 + Var(\hat f(x))
  \), so we can try to minimize the integrated $MSE$ over all points to find the best bandwidth.

  If we know $f$ and $f''$ we can compute the bias and variance analytically (up to some precision).
  From this we can derive global asymptotically optimal bandwidths, which can be useful if we can at least estimate $f$ and $f''$, and this can also be extended to local bandwidths.
\end{sectionbox}
\begin{sectionbox}[Density estimation in higher dimensions]\nospacing{}
  Basically use $\hat f(\vec{x}) = \frac{1}{nh^d}\sum_{i=1}^nK((\vec{x}-\vec{X}_i)/h)$ with a Kernel that supports vectors.
  The Gaussian kernel is the only one that is radially symmetric.
  Note that in higher dimensions, density estimation becomes very hard, due to data points becoming very sparse.
\end{sectionbox}


\section{Nonparametric Regression}\label{sec:nonparametric_regression}
Nonparametric regression with \emph{one} predictor variable, i.e. $Y_{i} = m(x_{i}) + \epsilon_{i}$ with $\epsilon_{1:n}$ i.i.d and $\mathbb{E}[\epsilon_{i}] = 0$. We want $m(x) = \mathbb{E}[Y|x]$ and ``some'' smoothness.

\subsection{The Kernel Regression Estimator}\label{subsec:kernel_regression_estimator}
\begin{sectionbox}[Nadaraya-Watson kernel estimator]\nospacing{}
  A ``locally weighted'' approach yields the NW kernel estimator
  \begin{equation}\label{eq:nw_regressor}
  \hat m(x) = \frac{\sum_{i=1}^{n} \omega_{i}Y_{i}}{\sum_{i}\omega_{i}} = \argmin_{m_{x} \in \mathbb{R}}\sum_{i=1}^{n}\omega_{i}{(Y_{i}-m_{x})}^{2}
  \end{equation}
  with $\omega_{i} = K\left(\frac{x_{i}-x}{h}\right)$ a kernel centered at $x_{i}$ and bandwidth $h$.
  As $h$ small $\rightarrow$ large then (high variance) $\rightarrow$ (high bias).
  For $x_{i}$ equidistant there exists $h_{\textrm opt} = f(\sigma_{\epsilon}^{2}, m''(x))$ which can be iteratively found.

\begin{mintlinebox}{R}
    ksmooth(x, y, kernel="normal", bandwidth=0.2, x.points=x)$y$
\end{mintlinebox}
%$
\begin{mintlinebox}{R}
  # automatic bandwidth
  fit.lo<-lokerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
  fit.gl<-glkerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
\end{mintlinebox}

Instead of finding a local constant $m_{x}$ we can also find a \emph{local polynomial}, i.e.\ we replace $m_{x}$ with $\beta_{1} + \sum_{i=2}^{p} \beta_{i}{(x_{i}-x)}^{i-1}$ (usually $p=2$ or $p=4$).
Often better at edges and yields first derivative.
  \begin{mintlinebox}{R}
  fit.loess <- loess(y ~ x, data=data.frame(x=x, y=y_pert), span=0.2971339)
  fit.loess.pred <- predict(fit.loess, newdata=x)
  \end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[The hat matrix $\mathcal{S}$]\nospacing{}
  We want to construct $\mathcal{S}$ with $\hat{\vec{Y}} = \mathcal{S}\vec{Y}$, i.e.\ the linear operator mapping the labels to the predictions.
  Given the regression (smoothing) function $s$, we compute $\mathcal{S}_{\cdot j} = s(\vec{x}, \vec{e}_{j}, h)$ with $\vec{e}_{j}$ the $j$-th unit vector.
  Then $Cov(\hat m(\vec{x})) = Cov(\mathcal{S} \vec{Y}) = \mathcal{S} Cov(\vec{Y}) \mathcal{S}^{\top} = \sigma_{\epsilon}^{2}\mathcal{S}\mathcal{S}^{\top}$, i.e. $Cov(\hat m(x_{i}), \hat m(x_{j})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S})}^{\top}_{ij}$, and $Var(\hat m(x_{i})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S}^{\top})}_{ii}$.
  We can estimate $\sigma_{\epsilon}^{2} \approx \sum_{i=1}^{n}{(Y_{i} - \hat m(x_{i}))}^{2}/(n-df)$.
  Then
  \begin{itemize}
    \item $\widehat{s.e.}(\hat m(x_{i})) = \sqrt{\widehat{Var}(\hat m(x_{i}))} = \hat \sigma_{\epsilon} \sqrt{{(\mathcal{S}\mathcal{S}^{\top})}_{ii}}$
    \item $\hat m(x_{i}) \approx \mathcal{N}\left(\mathbb{E}[\hat m(x_{i})], Var(\hat m(x_{i}))\right)$
    \item $I = \hat m(x_{i}) \pm 1.96 \cdot \widehat{s.e.}(\hat m(x_{i}))$ $\rightarrow$ (pointwise) CI
  \end{itemize}

  Additionally we can compute the \emph{degrees of freedom} for regression estimators with $df = \mathbf{tr}(\mathcal{S})$.

  \begin{mintlinebox}{R}
# Construct S matrix
N <- length(x); Eye <- diag(N)
S.nw <- S.lp <- S.ss <- matrix(0, nrow=N, ncol=N)
for (j in 1:N) {
  y_ <- Eye[, j]
  S.nw[, j] <- ksmooth(x, y_, kernel="normal", bandwidth=0.2, x.points=x)$y ;$}

est.nw<-est.lp<-est.ss<-matrix(0,nrow=length(x),ncol=nrep)
for (i in 1:nrep) {
  # generate y with disturbance
  y_pert <- y + rnorm(length(x), mean=0, sd=1)
  # try to fit NW
  est.nw[, i] <- ksmooth(x=x, y=y_pert, kernel="normal", bandwidth=0.2, x.points=x)$y ;$
  sig_sq.nw <- sum((y_pert - est.nw[, i])^2) / (length(y) - sum(diag(S.nw)))
  se.nw[, i] <- sqrt(sig_sq.nw * diag(S.nw \%*\% t(S.nw)))}

  \end{mintlinebox}


\end{sectionbox}
\subsection{Smoothing splines and penalized regression}\label{subsec:smoothing_splines_and_penalized_regression}
\begin{sectionbox}\nospacing{}
High-order polynomials do not work, so splines are used. We discuss splines \emph{without} having to specify the knots.
Find $\argmin_{m \in C^{0}(\mathbb{R})} \sum_{i=1}^{n}(Y_{i} - m(x_{i}))^{2} + \lambda \int_{\mathbb{R}}m''(z)^{2} dz$.
Note that the minimizer is \emph{finite dimensional} --- it is a cubic spline that can be computed using a set of basis functions $m_{\lambda}(x) = \sum_{j=1}^{n}\beta_{j}B_{j}(x)$ or $||\vec{Y} - B\vec{\beta}||^{2}+\lambda \vec{\beta}^{\top}\Omega\vec{\beta} \Rightarrow \hat{\vec{\beta}} = {(B^{\top}B + \lambda\Omega)}^{-1}B^{\top}\vec{Y}$.
Choose $\lambda$ on the scale of $df = \mathbf{tr}(\mathcal{S}_{\lambda})$.
Note that this is Ridge-type regression, which saves us from being overparametrized ($n$ points, $n$ parameters).
In the exam, this is \emph{not} considered ``standard'' least squares.
  \begin{mintlinebox}{R}
  fit.ss<-smooth.spline(x, y_pert, spar=0.6)
  fit.ss.pred[, i]<-predict(fit.ss, newdata=x)$y ; $
  \end{mintlinebox}
\end{sectionbox}

\section{Cross Validation}\label{sec:cross-validation}
Let $(X_{1}, Y_{1}), \dots, (X_{n}, Y_{n})$ i.i.d $\sim P$.
We would like to compute $\mathbb{E}_{(X_{\textrm new}, Y_{\textrm new})}[\rho(\vec{Y}_{\textrm new}, \hat{m}_{\textrm train} (X_{\textrm new}))]$.
\begin{sectionbox}[Constructing cross-validation datasets]\nospacing{}
  Approaches include
  \begin{description}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item[Validation set:] ---
    \item[Leave-one-out CV:] $n^{-1}\sum_{i=1}^{n}\rho\left(Y_{i}, \hat{m}_{n-1}^{(-i)}(X_{i})\right)$. Approximately unbiased.
    \item[$K$-fold CV:] $K^{-1}\sum_{i=1}^{K}|\mathcal{B}_{k}|^{-1}\sum_{i\in\mathcal{B}_{k}}\rho\left(Y_{i}, \hat{m}_{n-|\mathcal{B}_{k}|}^{(-\mathcal{B}_{k})}(X_{i})\right)$. Smaller variance than $1$-CV.
    \item[Random division:] Like $K$-fold, but build $\mathcal{B}_{k}$ by sampling without replacement ($\approx 10\%$). Usually fastest.
  \end{description}
\end{sectionbox}
\begin{sectionbox}[Tricks using hat matrix]\nospacing{}
  For linear fitting operators and the loss $\rho(y, x) = (y-x)^{2}$ we can exploit the hat matrix and get the full $1$CV result in a single step using
  \[
    n^{-1}\sum_{i=1}^{n}\left(Y_{i}-\hat{m}_{n-1}^{(-i)}(X_{i})\right)^{2} = n^{-1}\sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{m}(X_{i})}{1-\mathcal{S}_{ii}}\right)^{2}.
  \]
  It can be cheaper to just compute $\mathbf{tr}(\mathcal{S})$ (instead of all $\mathcal{S}_{ii}$), which leads to the \emph{generalized cross-validation}
  \[
    GCV = \frac{n^{-1}\sum_{i=1}^{n}(Y_{i}-\hat{m}(X_{i}))^{2}}{(1-n^{-1}\mathbf{tr}(\mathcal{S}))^{2}}.
  \]
  The two equations coincide if $\mathcal{S}_{ii}=c\ \forall i$.
\end{sectionbox}

\section{Bootstrap}\label{sec:bootstrap}
Efron's \emph{parametric} and \emph{nonparametric bootstrap} can be described as ``simulating from an estimated model'' and can be used for \emph{statistical inference (confidence intervals and testing)} and \emph{estimating the predictive power of a model or algorithm}.

\begin{sectionbox}[Nonparametric bootstrap]\nospacing{}
  Let $Z_{1:n}$ i.i.d $\sim P$ with $Z_i=(X_i, Y_i), X_i \in \mathbb{R}^p, Y_i \in \mathbb{R}$, and let $\hat \theta_n = g(Z_{1:n})$ be an estimator.
  We would like to know the \emph{distribution of $\hat \theta_n$}.
  We approximate $\vec{P}$ by the\emph{empirical distribution $\hat{\vec{P}}_n$} that assigns $\mathbb{P}[X_i] = 1/n\ \forall i$.
  Then we can repeatedly sample $Z_{1:n}^\ast$ i.i.d. $\sim \hat P_n$ and compute $\hat\theta_n^\ast = g(Z_{1:n}^\ast)$.
  The histogram (or any density estimator) then describes the distribution of $\hat{\theta}_n^\ast$.
  The algorithm reads
  \begin{enumeratenosep}
    \item Sample (with replacement) $Z_{1:n}^\ast$ i.i.d $\sim \hat P_n$.
    \item Compute the bootstrapped estimator $\hat\theta_n^\ast=g(Z_{1:n}^\ast)$.
    \item Repeat $B$ times to obtain $\hat \theta_n^{\ast 1:B}$.
    \item Approximate $\mathbb{E}^\ast[\hat\theta_n^\ast] \approx B^{-1}\sum_{i=1}^B\hat\theta_n^{\ast i}$ and $Var^\ast(\hat\theta_n^\ast) \approx (B-1)^{-1}\sum_{i=1}^B\left(\hat\theta_n^{\ast i} - B^{-1}\sum_{j=1}^B \hat\theta_n^{\ast j}\right)^2$.
      Then $\alpha$-quantile of $\hat\theta_n^\ast \approx $ empirical $\alpha$-quantile of $\hat\theta_n^{\ast 1:B}$.
  \end{enumeratenosep}
\end{sectionbox}

\begin{notebox}[Central limit theorem]\nospacing{}
  Let $X_i$ be a random variable with $\mathbb{E}[X_i] = 0$ and $Var(X_i) = \sigma^2$. Then $n^{-1}\sum_{i=1}^nX_i \overset{n\to \infty}{\to} \mathcal{N}(\mu, \sigma^2/n)$.
\end{notebox}
\begin{notebox}[Bootstrap consistence]\nospacing{}
  Consistency of the bootstrap typically holds if the limiting distribution of $\hat \theta_n$ is Normal and if $Z_{1:n}$ are i.i.d. 
  Mathematically, for an increasing sequence $a_n$ and $\forall x$, $\mathbb{P}[a_n(\hat\theta_n-\theta)\leq x] - \mathbb{P}^\ast[a_n(\hat\theta_n^\ast - \hat \theta_n) \leq x] \overset{P}\to 0 \text{ as } n \to \infty$.
  Then $Op^\ast(\hat\theta_n^\ast)/Op(\hat\theta_n) \overset{P}{\to} 1$ with $Op \in \{Var, \mathbb{E}\}$.
\end{notebox}

\begin{sectionbox}[Bootstrap confidence interval]\nospacing{}
  Given bootstrap consistence, we can compute $[\hat \theta_n - \hat q_{1-\alpha/2}, \hat\theta_n - \hat q_{\alpha/2}]$ with $\hat q_\alpha = \alpha$-(bootstrap) quantile of $\hat\theta_n^\ast - \hat \theta_n$.
\end{sectionbox}

\section{Classification}\label{sec:classification}
Given $(X_1, Y_1), \dots, (X_n,Y_n)$ i.i.d. with $Y_i \in \{0, \dots, J-1\}$, determine $\pi_j(x) = \mathbb{P}[Y=j|X=x]\ \forall j = 0,1,\dots,J-1$.
The optimal classifier is the \emph{Bayes classifier}, which is simply $\mathcal{C}_{\textrm Bayes}(x) = \argmax_{0\leq j\leq J-1}\pi_j(x)$.
Then, the zero-one test set error is called \emph{Bayes risk}, i.e. $\mathbb{P}[\mathcal{C}_{\textrm Bayes}(X_{\textrm new}) \neq Y_{\textrm new}]$.

\begin{sectionbox}[Discriminant analysis]\nospacing{}
  \textbf{Linear case: }Assume $(X | Y=j) \sim \mathcal{N}_p(\mu_j, \Sigma)$, $\mathbb{P}[Y=j] = p_j$, and $\sum_{j=0}^{J-1}p_j=1$.
  Then by Bayes formula $\pi_j(x) = \frac{f_{X|Y=j}(x)\cdot p_j}{\sum_{k=0}^{J-1}f_{X|Y=k}(x)\cdot p_k}$ with each $f_{X|Y=j}$ a Gaussian $\mathcal{N}(\mu_j, \Sigma_{(j)})$.
  We can estimate $\mu_j$ and even $\Sigma$/$\Sigma_j$ using closed formulas, but we also need priors for $Y_i$, which often is picked as $p_j=n_j/n$.
  This results in $\hat \delta_j(x) = (x-\hat{\mu}_j/2)^{\top}\Sigma^{-1}\hat{\mu}_j+\log(\hat p_j)$ with (linear in $x$) decision boundaries $\hat{\delta}_j(x) - \hat \delta_{j'}(x) \geq 0$ and $\mathcal{C}(x) = \argmax_j \hat \delta_j(x)$.

  \textbf{Quadratic case: } Now we assume different $\Sigma_j$ for each class and obtain quadratic decision boundaries $\hat{\delta}_j(x) = -\log(\det(\hat\Sigma_j))/2 - (x-\hat{\mu}_j)^{\top}\hat{\Sigma}_j^{-1}(x-\hat{\mu}_j)/2 + \log(\hat p_j)$.
  The price: $J\cdot p(p+1)/2$ parameters (for all $\Sigma$s) vs. $p(p+1)/2$ for a single $\Sigma$.
\end{sectionbox}

\begin{sectionbox}[Logistic regression for binary classification]\nospacing{}
  Given some model $g: \mathbb{R}^p \to \mathbb{R}$ (e.g. a linear model) we can use the logistic transform $\pi \mapsto \log(\pi/(1-\pi))$ to get probabilities: $\log(\pi(x)/(1-\pi(x))) = g(x)$ and $\pi(x) = 1/(1+\exp{(-g(x))})$.
  This implies $Y_i \sim \text{Bernoulli}(\pi(x_i))$ (e.g. weighted coin flip). The likelihood is $L(\vec{\beta}; (X_i,Y_i)_{i=1:n}) = \prod_{i=1}^n\pi(x_i)^{Y_i}(1-\pi(x_i))^{1-Y_i}$.
  We typically estimate $\vec{\beta}$ using e.g. (Newton's) gradient descent (due to a non-linear problem).
  As $n\to \infty$ we can asymptotically compute the standard errors $\widehat{s.e.}(\hat{\beta}_j)$ and t-test statistics $\hat\beta_j/\widehat{s.e.}(\hat\beta_j) \sim \mathcal{N}(0,1)$ (under $H){0,j}: \beta_j=0)$.
  \begin{mintlinebox}{R}
    fit <- glm(Y~., data=data, family="binomial")
    mean((predict(fit, type="response") > 0.5) == data$Y)$
  \end{mintlinebox}
\end{sectionbox}

\begin{notebox}[Linear predictors]\nospacing{}
  Note that both \emph{LDA} and \emph{Logistic regression} are \emph{linear} in the prediction variables.
  For LDA that comes from the Gaussian assumption (i.e. ``linearization'' of the true distribution), for Logistic regression it comes from the linear log-odds function.
\end{notebox}
\begin{notebox}[Multiclass case ($J>2$)]\nospacing{}
  \begin{enumeratenosep}
  \item $J$ classes $\rightarrow$ $J$ binary variables: $\tilde \pi_j(x) = \frac{\hat pi_j(x)}{\sum_{j=0}^{J-1}\hat\pi_j(x)}$
  \item Using \emph{multinomial distribution} (parametric linear logistic) (see \verb!multinom!)
  \item ``Reference class'' $\log(\pi_j(x)/\pi_0(x)) = g_j(x)$
  \item Pairwise 1-vs-1, fitting ${J \choose 2}\cdot p$ parameters
  \item Exploiting ``ordered'' classes with proportional odds
  \end{enumeratenosep}
\end{notebox}


\section{Flexible regression and classification methods}\label{sec:flexible_regression_and_classification_methods}
We fight the \emph{curse of dimensionality} by making some structural assumptions (although staying with methods $g(\cdot): \mathbb{R}^p \to \mathbb{R}$ of nonparametric nature).
\subsection{Additive models}%
\label{sub:additive_models}
\begin{sectionbox}\nospacing{}
  Decompose multivariate function in bias plus sum of univariate functions, i.e. $g_{\textrm add}: \mathbb{R}^p\to\mathbb{R}, x\mapsto g_{\textrm add}(x) = \mu + \sum_{j=1}^pg_j(x_j)$ with $g_j(\cdot): \mathbb{R}\to\mathbb{R}, \mathbb{E}[g_j(X_j)]=0$.
  Note that the zero-mean requirement for each $g_j(\cdot)$ makes the problem well posed.
  This approach is a generalization of linear models, and similarly can not model interaction terms $g_{j,k}(x_j, x_k)$.
  Due to the way they are constructed, additive linear models \emph{avoid the curse of dimensionality}!
\end{sectionbox}
\begin{sectionbox}\nospacing{}
  To construct the models, let $\mathcal{S}_j$ be a smoothing technique (e.g. \emph{Nadaraya-Watson Gaussian kernel estimators}).
  Then, the \textbf{backfitting} algorithm works as follows:
  \begin{itemizenosep}
    \item Compute $\hat \mu = n^{-1}\sum_{i=1}^n Y_i$ and initialize $\hat g_j(\cdot) \coloneqq 0$.
    \item Cycle through the indices $j = 1,2,\dots,p,1,2,\dots,p,1,2,\dots$ and update
      $\hat g_j=\mathcal{S}_j(\vec{Y}-\hat \mu\vec{1}-\sum_{k\neq j}\hat g_k)$.
      Stop each function at convergence.
    \item Normalize the functions: $\tilde g_j(\cdot) = \hat g_j(\cdot) - n^{-1}]sum_{i=1}^n\hat g_j(X_{ij})$.
  \end{itemizenosep}
  This basically makes the algorithm repeatedly solve the 1-dimensional fitting problem.
  The algorithm may be slow but often works and can use any 1-dimensional fitting technique.
\end{sectionbox}
\begin{sectionbox}\nospacing{}
  When fitting Additive models in R with the function \verb!gam!, the smoothers $\mathcal{S}_j$ penalized regression spline, and the degrees of freedom for each spline (i.e. each variable) will be determined through cross-validation.

  \begin{mintlinebox}{R}
    fit <- gam(Y ~ s(x1) + s(x2) + ..., data=data)
    plot(fit, pages=1, shade=TRUE)
    sfsmisc::TA.plot(fit, labels="o")
  \end{mintlinebox}
\end{sectionbox}

\subsection{Multivariate adaptive regression splines}%
\label{sub:multivariate_adaptive_regression_splines}



\subsection{Trees}%
\label{sub:trees}

\subsection{Ridge and Lasso}%
\label{sub:ridge_and_lasso}



\section{Bagging and Boosting}\label{sec:bagging_and_boosting}
\begin{sectionbox}[Bagging and Subbagging]\nospacing{}
  \textbf{B}ootstrap \textbf{agg}regat\textbf{ing} (bagging) (mostly on trees), uses $\hat g(\cdot): \mathbb{R}^{p}\to \mathbb{R}$ and ensembles them (which comes at the loss of interpretability).
  \begin{enumeratenosep}[label=\roman*]
    \item Generate bootstrap sample $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{n}^{\ast}, Y_{n}^{\ast})$ and compute $\hat{g}^{\ast}_{i=1}(\cdot)$. Repeat $B$ times.
    \item Aggregate bootstrap estimates with $\hat{g}_{\textrm Bag}(\cdot) = B^{-1}\sum_{i=1}^{B} \hat{g}^{\ast}_{i}(\cdot) \approx \mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)]$.
  \end{enumeratenosep}
  Note that $\hat{g}_{\textrm Bag}(\cdot) = \hat{g}(\cdot) + \underbrace{(\mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)] -\hat{g}(\cdot))}_{\text{bootstrap bias estimate}}$.
  We can reduce variance at price of higher bias (at least for trees).
  In fact, for many $x$, $Var(\hat{g}_{\textrm Bag}(x)) < Var(\hat{g}(x))$. We can use larger trees (higher variance) to balance the bias-variance trade-off.

  For \textbf{Sub}sample \textbf{agg}regat\textbf{ing} (Subbagging), we draw $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{m}^{\ast}, Y_{m}^{\ast})$ without replacement (e.g. with $m = \lfloor n/2\rfloor$), which can be cheaper overall and is equivalent to Bagging in some simple settings.
\end{sectionbox}

\begin{sectionbox}[$L_{2}$Boosting]\nospacing{}
  Similar to Bagging, iterates on a ``base-learner'' by continually adding a fit on the residuals.
  \begin{enumeratenosep}[label=\roman*]
    \item Get first fit $\hat{g}_{1}(\cdot)$ by fitting on the full data. Compute residuals $U_{i} = Y_{i} - \hat{g}_{1}(X_{i})$ and let $\hat{f}_{1}(\cdot) = \nu \hat{g}_{1}(\cdot)$ with $0 < \nu \leq 1$ (typically $\nu = 0.1$).
    \item For $m = 2, 3, \dots, M$ fit $\hat{g}_{m}(\cdot)$ on residuals $U_{i}$ and set $\hat{f}_{m}(\cdot) = \hat{f}_{m-1}(\cdot) + \nu \hat{g}_{m}(\cdot)$ (and update residuals using $\hat{f}_{m}(\cdot)$).
  \end{enumeratenosep}
  The main tuning parameter is the stopping point $M$. Boosting \emph{increases} the bias and can be used to ensemble trees to fit more complex data. See e.g.
  \begin{mintlinebox}{R}
      ?mboost; ?xgboost; ?gbm;
  \end{mintlinebox}
\end{sectionbox}

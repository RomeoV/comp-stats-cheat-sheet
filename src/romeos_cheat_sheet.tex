\section{Multiple Linear Regression}\label{sec:multiple_linear_regression}
\subsection{The Linear Model}\label{subsec:the_linear_model}
\begin{sectionbox}\nospacing{}
Assume $Y_i = x_{i}^{\top}\beta + \epsilon_{i}$ or $Y = X \times \beta + \epsilon$ with $X \in \mathcal{R}^{(n \times p)}; (n > p)$ and $\mathbb{E}[\epsilon_{i}]=0, Var(\epsilon_{i}) = \sigma^{2}$.
$X$ is often augmented with $(1_{N\times 1})$ to use $\beta_{1}$ as bias.
\end{sectionbox}
\subsection{Least Squares Method}\label{subsec:least_squares_method}
\begin{sectionbox}\nospacing{}
  Least squares estimator is $\hat \beta = \argmin_{\beta} ||Y - X\beta||^{2}_{2} = {(X^{\top}X)}^{-1} X^{\top}Y$ (orthogonal projection of $Y$ onto $span(X)$).
  Estimate $\hat{\sigma}^{2} = \frac{1}{n-p}\sum_{i=1}^{n}r_{i}^{2}$ with $\mathbb{E}[\hat{\sigma}^{2}] = \sigma^{2}$.
\end{sectionbox}

\begin{notebox}[Assumptions for Linear Model]\nospacing{}
    \begin{enumeratenosep}[label=\roman*]
        \item Linear regression equation is correct, i.e. $\mathbb{E}[\epsilon_{i}]=0\ \forall i$.
        \item We measure $x_{i}$'s exactly. Else, need correction (?).
        \item Error is homoscedastic, i.e. $Var(\epsilon_{i})=\sigma^{2}\ \forall i$. Else, use ``Weighted LS''.
        \item Errors are uncorrelated, i.e. $Cov(\epsilon_{i}, \epsilon_{j}) = 0\ \forall i \neq j$. Else ``Generalized LS''.
        \item Errors are jointly normally distributed. Else ``Robust Methods''.
    \end{enumeratenosep}
\end{notebox}

\begin{notebox}[Moments of least squares estimates]\nospacing{}
  Assume $\vec{Y}=X\vec{\beta} + \epsilon, \mathbb{E}[\vec{\epsilon}]=\vec{0},\ Cov(\vec{\epsilon} \vec{\epsilon}^{\top}) = \sigma^{2}I$ (all assumptions satisfied). Then
  \begin{enumeratenosep}[label=\roman*]
    \item $\mathbb{E}[\hat{\vec{\beta}}] = \vec{\beta}$ ($\hat{\vec{\beta}}$ is unbiased).
    \item $\mathbb{E}[\hat{\vec{Y}}] = \mathbb{E}[\vec{Y}] = X\vec{\beta}$ and $\mathbb{E}[\vec{r}] = \vec{0}$.
    \item $Cov(\hat{\vec{\beta}}) = \sigma^{2}{(X^{\top}X)}^{-1}$.
    \item $Cov(\hat{\vec{Y}}) = \sigma^{2} P, Cov(\vec{r}) = \sigma^{2}(I-P)$.
  \end{enumeratenosep}
  If additionally $\epsilon_{i}, \dots, \epsilon_{n} \text{ i.i.d. } \sim \mathcal{N}(0, \sigma^{2})$, then
  \begin{enumeratenosep}[label=\roman*]
    \item $\hat{\vec{\beta}} \sim \mathcal{N}_{p}{(\vec{\beta}, \sigma^{2}(X^{\top}X))}^{-1}$
    \item $\hat{\vec{Y}} \sim \mathcal{N}_{n}(X\vec{\beta}, \sigma^{2}),\ \vec{r} \sim \mathbb{N}_{n}(\vec{0}, \sigma^{2}(I-P))$
    \item $\hat \sigma^{2} \sim \frac{\sigma^{2}}{n-p}\chi^{2}_{n-p}$.
  \end{enumeratenosep}
  Even when normality assumption doesn't hold, central limit theorem is a justification.
\end{notebox}
\subsection{Tests and Confidence Regions}\label{subsec:tests_and_confidence_regions}
\begin{sectionbox}[T-test]\nospacing{}
  Assume linear model with Gaussian errors (or ``large enough'' sample size), s.t. $\hat{\vec{\beta}} \sim \mathcal{N}_{p}\left(\vec{\beta}, \sigma^{2}{(X^{\top}X)}^{-1}\right)$ is normally distributed.
  Then we can test the null-hypothesis $H_{0,j}: \beta_{j = 0}$ against $H_{A,j}: \beta_{j \neq 0}$:
  \[\frac{\hat \beta_{j}}{\sqrt{\sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim \mathcal{N}(0,1)\] under the null-hypothesis $H_{0,j}$. Since $\sigma^{2}$ is unknown, replace by $\hat \sigma^{2} \rightarrow$ T-test:
  \[T_{j} = \frac{\hat \beta_{j}}{\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim t_{n-p}\] under the null-hypothesis $H_{0,j}$. Note that $t_{n-p} \approx \mathcal{N}$.
\end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 As individual t-test for $H_{0,j}$ gives the effect of $\beta_{j}$ after subtracting the linear effect of all $\beta_{i\neq j}$.
\end{attentionbox}

 \begin{sectionbox}[Global null hypothesis and ANOVA]\nospacing{}
  We can also check the global null-hypothesis $H_{0}: \beta_{2} = \cdots = \beta_{p} = 0$ using the \emph{analysis of variance} (ANOVA), which decomposes
  \[||\vec{Y} - \bar{\vec{Y}}||^{2}_{2} = ||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}_{2} + ||\vec{Y} - \hat{\vec{Y}}||^{2}_{2}.\]
  Under the global null-hypothesis $\mathbb{E}[\vec{Y}] = \mathcal{E}[\bar{\vec{Y}}] = const$. (no effect of predictor variables). Basically dividing $\sigma^{2}$ by $\hat \sigma^{2}$ yields F-statistic:
  \[F = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}/(p-1)}{||\vec{Y} - \hat{\vec{Y}}||^{2} / (n-p)} \sim F_{p-1,n-p}\] under the global null-hypothesis $H_{0}$.
  ANOVA also yields \emph{goodness of fit} $R^{2} = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}}{||\vec{Y} - \bar{\vec{Y}}||^{2}}$, which should be around $1$.
  Finally, we can also build a confidence interval using $\hat \beta_{j} \pm \sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} \cdot t_{n-p;1-\alpha/2}$.
 \end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 In \verb!summary.lm!, the term \emph{Std. Error} is $\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} = \sqrt{\hat{Var}(\hat \beta_{j})}$.
\end{attentionbox}
\todo[inline]{ANOVA function for determining if a model is worth it.}

\subsection{Checking Model Assumptions}\label{subsec:checking_model_assumptions}
\begin{sectionbox}[Tukey-Anscome Plot]\nospacing{}
  Error should fluctuate randomly. If error increases linearly, do log-transform $\vec{Y} \mapsto \log{\vec{Y}}$. If error increases with $\sqrt{Y}$, do a square-root-transform $\vec{Y} \mapsto \sqrt{\vec{Y}}$.
\end{sectionbox}
\begin{sectionbox}[QQ-Plot/Normal-Plot]\nospacing{}
  Plot empirical quantiles of residuals on y versus the theoretical quantiles of $\mathcal{N}(0,1)$ on x.
  If assumption holds, get straight line with intercept $\mu$ and slope $\sigma$.
  Z-shape: long-tailed distr.; Curved: skewed distr.
\end{sectionbox}

\subsection{Model Selection}\label{subsec:model_selection}
\begin{sectionbox}\nospacing{}
  Assume again $\mathbb{E}[\epsilon_{i}] = 0, Var(\epsilon_{i} = \sigma^{2})$.
  We need to adress \emph{bias-variance trade-off}.
  Bias is defined as $\mathbb{E} [\hat f(x)] - f(x)$, variance as $q/n \cdot \sigma^{2}$ with $q \leq p$.
\end{sectionbox}
\begin{sectionbox}[Mallows $C_{p}$ statistic]\nospacing{}
  Let $SSE(\mathcal{M})$ the residual sum of squares.
  Then $n^{-1} \sum_{i=1}^{n} \mathbb{E}\left[{(f(x) - \hat f_{\mathcal{I}}(x))}^{2}\right] \approx n^{-1}SSE(\mathcal{M})-\hat \sigma^{2} + 2\hat\sigma^{2}|\mathcal{M}|/n$, with $\mathcal{I}$ the indices of selected predictors and $|\mathcal{I}| = q$.
  Thus, we search for the model that minimizes the $C_{p}$-statistic with $C_{p}(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat \sigma^{2}} - n + 2|\mathcal{M}|$.
  Otherwise Akaike's information criterion (AIC) or Bayesian information criterion (BIC). AIC is equivalent to $C_{p}$ for linear Gaussian models.
  \begin{mintlinebox}{R}
    require(leaps); fit.all <- regsubsets(y~., data=data)
    p.regsubsets(fit.all)
  \end{mintlinebox}
\end{sectionbox}
\begin{sectionbox}[Forwards and backwards selection]\nospacing{}
  \begin{description}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item[Forward selection] (i) Start with empty model. (ii) (Greedily) Keep adding variable that reduces the residual sum of squares. (iii) When done, pick submodel which minimizes $C_{p}$.
    \item[Backward selection] (i) Start with full model. (ii) (Greedily) Keep excluding predictor that increases the residual sum of squares the least. (iii) When done, pick submodel which minimizes $C_{p}$.
  \end{description}
  Backwards selection typically better but more expensive. When $p \geq n$, use forward selection.
  Both methods prone to overfitting --- p-values (and similar values) are \emph{not} valid anymore and effects look too significant.
  \begin{mintlinebox}{R}
    fit.empty <- lm(y~1, data=data)
    fit.full <- lm(y~., data=data)
    fit.bw <- step(fit.full, direction="backward")
    fit.fw <- step(fit.full, direction="forward")
  \end{mintlinebox}

\end{sectionbox}


\section{Nonparametric Regression}\label{sec:nonparametric_regression}
Nonparametric regression with \emph{one} predictor variable, i.e. $Y_{i} = m(x_{i}) + \epsilon_{i}$ with $\epsilon_{1:n}$ i.i.d and $\mathbb{E}[\epsilon_{i}] = 0$. We want $m(x) = \mathbb{E}[Y|x]$ and ``some'' smoothness.

\subsection{The Kernel Regression Estimator}\label{subsec:kernel_regression_estimator}
\begin{sectionbox}[Nadaraya-Watson kernel estimator]\nospacing{}
  A ``locally weighted'' approach yields the NW kernel estimator
  \begin{equation}\label{eq:nw_regressor}
  \hat m(x) = \frac{\sum_{i=1}^{n} \omega_{i}Y_{i}}{\sum_{i}\omega_{i}} = \argmin_{m_{x} \in \mathbb{R}}\sum_{i=1}^{n}\omega_{i}{(Y_{i}-m_{x})}^{2}
  \end{equation}
  with $\omega_{i} = K\left(\frac{x_{i}-x}{h}\right)$ a kernel centered at $x_{i}$ and bandwidth $h$.
  As $h$ small $\rightarrow$ large then (high variance) $\rightarrow$ (high bias).
  For $x_{i}$ equidistant there exists $h_{\textrm opt} = f(\sigma_{\epsilon}^{2}, m''(x))$ which can be iteratively found.

\begin{mintlinebox}{R}
    ksmooth(x, y, kernel="normal", bandwidth=0.2, x.points=x)$y
\end{mintlinebox}
%$
\begin{mintlinebox}{R}
  # automatic bandwidth
  fit.lo<-lokerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
  fit.gl<-glkerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
\end{mintlinebox}

Instead of finding a local constant $m_{x}$ we can also find a \emph{local polynomial}, i.e.\ we replace $m_{x}$ with $\beta_{1} + \sum_{i=2}^{p} \beta_{i}{(x_{i}-x)}^{i-1}$ (usually $p=2$ or $p=4$).
Often better at edges and yields first derivative.
  \begin{mintlinebox}{R}
  fit.loess <- loess(y ~ x, data=data.frame(x=x, y=y_pert), span=0.2971339)
  fit.loess.pred <- predict(fit.loess, newdata=x)
  \end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[The hat matrix $\mathcal{S}$]\nospacing{}
  We want to construct $\mathcal{S}$ with $\hat{\vec{Y}} = \mathcal{S}\vec{Y}$, i.e.\ the linear operator mapping the labels to the predictions.
  Given the regression (smoothing) function $s$, we compute $\mathcal{S}_{\cdot j} = s(\vec{x}, \vec{e}_{j}, h)$ with $\vec{e}_{j}$ the $j$-th unit vector.
  Then $Cov(\hat m(\vec{x})) = Cov(\mathcal{S} \vec{Y}) = \mathcal{S} Cov(\vec{Y}) \mathcal{S}^{\top} = \sigma_{\epsilon}^{2}\mathcal{S}\mathcal{S}^{\top}$, i.e. $Cov(\hat m(x_{i}), \hat m(x_{j})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S})}^{\top}_{ij}$, and $Var(\hat m(x_{i})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S}^{\top})}_{ii}$.
  We can estimate $\sigma_{\epsilon}^{2} \approx \sum_{i=1}^{n}{(Y_{i} - \hat m(x_{i}))}^{2}/(n-df)$.
  Then
  \begin{itemize}
    \item $\widehat{s.e.}(\hat m(x_{i})) = \sqrt{\widehat{Var}(\hat m(x_{i}))} = \hat \sigma_{\epsilon} \sqrt{{(\mathcal{S}\mathcal{S}^{\top})}_{ii}}$
    \item $\hat m(x_{i}) \approx \mathcal{N}\left(\mathbb{E}[\hat m(x_{i})], Var(\hat m(x_{i}))\right)$
    \item $I = \hat m(x_{i}) \pm 1.96 \cdot \widehat{s.e.}(\hat m(x_{i}))$ $\rightarrow$ (pointwise) CI
  \end{itemize}

  Additionally we can compute the \emph{degrees of freedom} for regression estimators with $df = \mathbf{tr}(\mathcal{S})$.

  \begin{mintlinebox}{R}
# Construct S matrix
N <- length(x); Eye <- diag(N)
S.nw <- S.lp <- S.ss <- matrix(0, nrow=N, ncol=N)
for (j in 1:N) {
  y_ <- Eye[, j]
  S.nw[, j] <- ksmooth(x, y_, kernel="normal", bandwidth=0.2, x.points=x)$y ;$}

est.nw<-est.lp<-est.ss<-matrix(0,nrow=length(x),ncol=nrep)
for (i in 1:nrep) {
  # generate y with disturbance
  y_pert <- y + rnorm(length(x), mean=0, sd=1)
  # try to fit NW
  est.nw[, i] <- ksmooth(x=x, y=y_pert, kernel="normal", bandwidth=0.2, x.points=x)$y ;$
  sig_sq.nw <- sum((y_pert - est.nw[, i])^2) / (length(y) - sum(diag(S.nw)))
  se.nw[, i] <- sqrt(sig_sq.nw * diag(S.nw \%*\% t(S.nw)))}

  \end{mintlinebox}


\end{sectionbox}
\subsection{Smoothing splines and penalized regression}\label{subsec:smoothing_splines_and_penalized_regression}
\begin{sectionbox}\nospacing{}
High-order polynomials do not work, so splines are used. We discuss splines \emph{without} having to specify the knots.
Find $\argmin_{m \in C^{0}(\mathbb{R})} \sum_{i=1}^{n}(Y_{i} - m(x_{i}))^{2} + \lambda \int_{\mathbb{R}}m''(z)^{2} dz$.
Note that the minimizer is \emph{finite dimensional} --- it is a cubic spline that can be computed using a set of basis functions $m_{\lambda}(x) = \sum_{j=1}^{n}\beta_{j}B_{j}(x)$ or $||\vec{Y} - B\vec{\beta}||^{2}+\lambda \vec{\beta}^{\top}\Omega\vec{\beta} \Rightarrow \hat{\vec{\beta}} = {(B^{\top}B + \lambda\Omega)}^{-1}B^{\top}\vec{Y}$.
Choose $\lambda$ on the scale of $df = \mathbf{tr}(\mathcal{S}_{\lambda})$.
Note that this is Ridge-type regression, which saves us from being overparametrized ($n$ points, $n$ parameters).
In the exam, this is \emph{not} considered ``standard'' least squares.
  \begin{mintlinebox}{R}
  fit.ss<-smooth.spline(x, y_pert, spar=0.6)
  fit.ss.pred[, i]<-predict(fit.ss, newdata=x)$y ; $
  \end{mintlinebox}
\end{sectionbox}

\section{Cross Validation}\label{sec:cross-validation}
Let $(X_{1}, Y_{1}), \dots, (X_{n}, Y_{n})$ i.i.d $\sim P$.
We would like to compute $\mathbb{E}_{(X_{\textrm new}, Y_{\textrm new})}[\rho(\vec{Y}_{\textrm new}, \hat{m}_{\textrm train} (X_{\textrm new}))]$.
\begin{sectionbox}[Constructing cross-validation datasets]\nospacing{}
  Approaches include
  \begin{description}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item[Validation set:] ---
    \item[Leave-one-out CV:] $n^{-1}\sum_{i=1}^{n}\rho\left(Y_{i}, \hat{m}_{n-1}^{(-i)}(X_{i})\right)$. Approximately unbiased.
    \item[$K$-fold CV:] $K^{-1}\sum_{i=1}^{K}|\mathcal{B}_{k}|^{-1}\sum_{i\in\mathcal{B}_{k}}\rho\left(Y_{i}, \hat{m}_{n-|\mathcal{B}_{k}|}^{(-\mathcal{B}_{k})}(X_{i})\right)$. Smaller variance than $1$-CV.
    \item[Random division:] Like $K$-fold, but build $\mathcal{B}_{k}$ by sampling without replacement ($\approx 10\%$). Usually fastest.
  \end{description}
\end{sectionbox}
\begin{sectionbox}[Tricks using hat matrix]\nospacing{}
  For linear fitting operators and the loss $\rho(y, x) = (y-x)^{2}$ we can exploit the hat matrix and get the full $1$CV result in a single step using
  \[
    n^{-1}\sum_{i=1}^{n}\left(Y_{i}-\hat{m}_{n-1}^{(-i)}(X_{i})\right)^{2} = n^{-1}\sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{m}(X_{i})}{1-\mathcal{S}_{ii}}\right)^{2}.
  \]
  It can be cheaper to just compute $\mathbf{tr}(\mathcal{S})$ (instead of all $\mathcal{S}_{ii}$), which leads to the \emph{generalized cross-validation}
  \[
    GCV = \frac{n^{-1}\sum_{i=1}^{n}(Y_{i}-\hat{m}(X_{i}))^{2}}{(1-n^{-1}\mathbf{tr}(\mathcal{S}))^{2}}.
  \]
  The two equations coincide if $\mathcal{S}_{ii}=c\ \forall i$.
\end{sectionbox}

\section{Bootstrap}\label{sec:bootstrap}

\section{Classification}\label{sec:classification}

\section{Flexible regression and classification methods}\label{sec:flexible_regression_and_classification_methods}

\section{Bagging and Boosting}\label{sec:bagging_and_boosting}
\begin{sectionbox}[Bagging and Subbagging]\nospacing{}
  \textbf{B}ootstrap \textbf{agg}regat\textbf{ing} (bagging) (mostly on trees), uses $\hat g(\cdot): \mathbb{R}^{p}\rightarrow \mathbb{R}$ and ensembles them (which comes at the loss of interpretability).
  \begin{enumeratenosep}[label=\roman*]
    \item Generate bootstrap sample $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{n}^{\ast}, Y_{n}^{\ast})$ and compute $\hat{g}^{\ast}_{i=1}(\cdot)$. Repeat $B$ times.
    \item Aggregate bootstrap estimates with $\hat{g}_{\textrm Bag}(\cdot) = B^{-1}\sum_{i=1}^{B} \hat{g}^{\ast}_{i}(\cdot) \approx \mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)]$.
  \end{enumeratenosep}
  Note that $\hat{g}_{\textrm Bag}(\cdot) = \hat{g}(\cdot) + \underbrace{(\mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)] -\hat{g}(\cdot))}_{\text{bootstrap bias estimate}}$.
  We can reduce variance at price of higher bias (at least for trees).
  In fact, for many $x$, $Var(\hat{g}_{\textrm Bag}(x)) < Var(\hat{g}(x))$. We can use larger trees (higher variance) to balance the bias-variance trade-off.

  For \textbf{Sub}sample \textbf{agg}regat\textbf{ing} (Subbagging), we draw $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{m}^{\ast}, Y_{m}^{\ast})$ without replacement (e.g. with $m = \lfloor n/2\rfloor$), which can be cheaper overall and is equivalent to Bagging in some simple settings.
\end{sectionbox}

\begin{sectionbox}[$L_{2}$Boosting]\nospacing{}
  Similar to Bagging, iterates on a ``base-learner'' by continually adding a fit on the residuals.
  \begin{enumeratenosep}[label=\roman*]
    \item Get first fit $\hat{g}_{1}(\cdot)$ by fitting on the full data. Compute residuals $U_{i} = Y_{i} - \hat{g}_{1}(X_{i})$ and let $\hat{f}_{1}(\cdot) = \nu \hat{g}_{1}(\cdot)$ with $0 < \nu \leq 1$ (typically $\nu = 0.1$).
    \item For $m = 2, 3, \dots, M$ fit $\hat{g}_{m}(\cdot)$ on residuals $U_{i}$ and set $\hat{f}_{m}(\cdot) = \hat{f}_{m-1}(\cdot) + \nu \hat{g}_{m}(\cdot)$ (and update residuals using $\hat{f}_{m}(\cdot)$).
  \end{enumeratenosep}
  The main tuning parameter is the stopping point $M$. Boosting \emph{increases} the bias and can be used to ensemble trees to fit more complex data. See e.g.
  \begin{mintlinebox}{R}
      ?mboost; ?xgboost; ?gbm;
  \end{mintlinebox}
\end{sectionbox}

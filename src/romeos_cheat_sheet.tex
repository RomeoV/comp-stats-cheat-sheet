\noindent Summary for \emph{Computational Statistics 2021 @ ETH}\\
by Romeo Valentin (\url{github.com/RomeoV})
\section{Multiple Linear Regression}\label{sec:multiple_linear_regression}
\subsection{The Linear Model}\label{subsec:the_linear_model}
\begin{sectionbox}\nospacing{}
Assume $Y_i = x_{i}^{\top}\beta + \epsilon_{i}$ or $Y = X \times \beta + \epsilon$ with $X \in \mathcal{R}^{(n \times p)}; (n > p)$ and $\mathbb{E}[\epsilon_{i}]=0, Var(\epsilon_{i}) = \sigma^{2}$.
$X$ is often augmented with $(1_{N\times 1})$ to use $\beta_{1}$ as bias.
\end{sectionbox}
\subsection{Least Squares Method}\label{subsec:least_squares_method}
\begin{sectionbox}\nospacing{}
  LS estimator is $\hat \beta = \argmin_{\beta} ||Y - X\beta||^{2}_{2} = {(X^{\top}X)}^{-1} X^{\top}Y = PY$ (orth. proj. of $Y$ onto $span(X)$).
  Estimate $\hat{\sigma}^{2} = \frac{1}{n-p}\sum_{i=1}^{n}r_{i}^{2}$ with $\mathbb{E}[\hat{\sigma}^{2}] = \sigma^{2}$.
\end{sectionbox}

\begin{notebox}[Assumptions for Linear Model]\nospacing{}
    \begin{enumeratenosep}[label=\roman*]
        \item Linear regression equation is correct, i.e. $\mathbb{E}[\epsilon_{i}]=0\ \forall i$.
        \item We measure $x_{i}$'s exactly. Else, need correction (?).
        \item Error is homoscedastic, i.e. $Var(\epsilon_{i})=\sigma^{2}\ \forall i$. Else, use ``Weighted LS''.
        \item Errors are uncorrelated, i.e. $Cov(\epsilon_{i}, \epsilon_{j}) = 0\ \forall i \neq j$. Else ``Generalized LS''.
        \item Errors are jointly normally distributed. Else ``Robust Methods''.
    \end{enumeratenosep}
\end{notebox}

\begin{notebox}[Moments of least squares estimates]\nospacing{}
  Assume $\vec{Y}=X\vec{\beta} + \epsilon, \mathbb{E}[\vec{\epsilon}]=\vec{0},\ Cov(\vec{\epsilon} \vec{\epsilon}^{\top}) = \sigma^{2}I$ (all assumptions satisfied). Then
  \begin{enumeratenosep}[label=\roman*]
    \item $\mathbb{E}[\hat{\vec{\beta}}] = \vec{\beta}$ ($\hat{\vec{\beta}}$ is unbiased).
    \item $\mathbb{E}[\hat{\vec{Y}}] = \mathbb{E}[\vec{Y}] = X\vec{\beta}$ and $\mathbb{E}[\vec{r}] = \vec{0}$.
    \item $Cov(\hat{\vec{\beta}}) = \sigma^{2}{(X^{\top}X)^{-1}}$.
    \item $Cov(\hat{\vec{Y}}) = \sigma^{2} P, Cov(\vec{r}) = \sigma^{2}(I-P)$.
  \end{enumeratenosep}
  If additionally $\epsilon_{i}, \dots, \epsilon_{n} \text{ i.i.d. } \sim \mathcal{N}(0, \sigma^{2})$, then
  \begin{enumeratenosep}[label=\roman*]
    \item $\hat{\vec{\beta}} \sim \mathcal{N}_{p}{(\vec{\beta}, \sigma^{2}(X^{\top}X)^{-1})}$
    \item $\hat{\vec{Y}} \sim \mathcal{N}_{n}(X\vec{\beta}, \sigma^{2}),\ \vec{r} \sim \mathbb{N}_{n}(\vec{0}, \sigma^{2}(I-P))$
    \item $\hat \sigma^{2} \sim \frac{\sigma^{2}}{n-p}\chi^{2}_{n-p}$.
  \end{enumeratenosep}
  Even when normality assumption doesn't hold, central limit theorem is a justification.
\end{notebox}
\subsection{Tests and Confidence Regions}\label{subsec:tests_and_confidence_regions}
\begin{sectionbox}[T-test]\nospacing{}
  Assume linear model with Gaussian errors (or ``large enough'' sample size), s.t. $\hat{\vec{\beta}} \sim \mathcal{N}_{p}\left(\vec{\beta}, \sigma^{2}{(X^{\top}X)}^{-1}\right)$ is normally distributed.
  Then we can test the null-hypothesis $H_{0,j}: \beta_{j = 0}$ against $H_{A,j}: \beta_{j \neq 0}$:
  \[\frac{\hat \beta_{j}}{\sqrt{\sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim \mathcal{N}(0,1) \Rightarrow  T_{j} = \frac{\hat \beta_{j}}{\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim t_{n-p}\ \] under the null-hypothesis $H_{0,j}$. Unknown $\sigma^{2}$ is replaced by $\hat \sigma^{2}$. Note that $t_{n-p} \approx \mathcal{N}$.
\end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 As individual t-test for $H_{0,j}$ gives the effect of $\beta_{j}$ after subtracting the linear effect of all $\beta_{i\neq j}$.
\end{attentionbox}

 \begin{sectionbox}[Global null hypothesis and ANOVA]\nospacing{}
  We can also check the global null-hypothesis $H_{0}: \beta_{2} = \cdots = \beta_{p} = 0$ using the \emph{analysis of variance} (ANOVA), which decomposes
  \[||\vec{Y} - \bar{\vec{Y}}||^{2}_{2} = ||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}_{2} + ||\vec{Y} - \hat{\vec{Y}}||^{2}_{2}.\]
  Under the global null-hypothesis $\mathbb{E}[\vec{Y}] = \mathcal{E}[\bar{\vec{Y}}] = const$. (no effect of predictor variables). $\sigma^{2}/ \hat \sigma^{2}$ yields F-statistic:
  \[F = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}/(p-1)}{||\vec{Y} - \hat{\vec{Y}}||^{2} / (n-p)} \sim F_{p-1,n-p}\] under the global null-hypothesis $H_{0}$.
  ANOVA also yields \emph{goodness of fit} $R^{2} = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}}{||\vec{Y} - \bar{\vec{Y}}||^{2}}$, which should be around $1$.
  Finally, we can also build a confidence interval using $\hat \beta_{j} \pm \sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} \cdot t_{n-p;1-\alpha/2}$.
   \begin{mintlinebox}{R}
  	anova(fit) # global F test
  	# partial F test - sig. of predictors in .full but not .part
  	anova(fit.full, fit.part)
  \end{mintlinebox}
  
  
 \end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 In \verb!summary.lm!, the term \emph{Std. Error} is $\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} = \sqrt{\hat{Var}(\hat \beta_{j})}$.
\end{attentionbox}


\subsection{Checking Model Assumptions}\label{subsec:checking_model_assumptions}
\begin{sectionbox}[Tukey-Anscome Plot]\nospacing{}
  Error should fluctuate randomly. If error increases linearly, do log-transform $\vec{Y} \mapsto \log{\vec{Y}}$. If error increases with $\sqrt{Y}$, do a square-root-transform $\vec{Y} \mapsto \sqrt{\vec{Y}}$.
\end{sectionbox}
\begin{sectionbox}[QQ-Plot/Normal-Plot]\nospacing{}
  Plot empirical quantiles of residuals on y versus the theoretical quantiles of $\mathcal{N}(0,1)$ on x.
  If assumption holds, get straight line with intercept $\mu$ and slope $\sigma$.
  Z-shape: long-tailed distr.; Curved: skewed distr.
\end{sectionbox}

\subsection{Model Selection}\label{subsec:model_selection}
\begin{sectionbox}\nospacing{}
  Assume again $\mathbb{E}[\epsilon_{i}] = 0, Var(\epsilon_{i} = \sigma^{2})$.
  We need to address \emph{bias-variance trade-off}.
  Bias is defined as $\mathbb{E} [\hat f(x)] - f(x)$, variance as $q/n \cdot \sigma^{2}$ with $q \leq p$.
\end{sectionbox}
\begin{sectionbox}[Mallows $C_{p}$ statistic]\nospacing{}
  Let $SSE(\mathcal{M})$ the residual sum of squares.
  Then $n^{-1} \sum_{i=1}^{n} \mathbb{E}\left[{(f(x) - \hat f_{\mathcal{I}}(x))}^{2}\right] \approx n^{-1}SSE(\mathcal{M})-\hat \sigma^{2} + 2\hat\sigma^{2}|\mathcal{M}|/n$, with $\mathcal{I}$ the indices of selected predictors and $|\mathcal{I}| = q$.
  Thus, we search for the model that minimizes the $C_{p}$-statistic with $C_{p}(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat \sigma^{2}} - n + 2|\mathcal{M}|$.
  Otherwise Akaike's information criterion (AIC) or Bayesian information criterion (BIC). AIC is equivalent to $C_{p}$ for linear Gaussian models.
  \begin{mintlinebox}{R}
    require(leaps); fit.all <- regsubsets(y~., data=data)
    p.regsubsets(fit.all)
  \end{mintlinebox}
\end{sectionbox}
\begin{sectionbox}[Forwards and backwards selection]\nospacing{}
  \textbf{Forward selection:} (i) Start with empty model. (ii) (Greedily) Keep adding variable that reduces the residual sum of squares. (iii) When done, pick submodel which minimizes $C_{p}$.

  \textbf{Backward selection:} (i) Start with full model. (ii) (Greedily) Keep excluding predictor that increases the residual sum of squares the least. (iii) When done, pick submodel which minimizes $C_{p}$.
  Backwards selection typically better but more expensive. When $p \geq n$, use forward selection.
  Both methods prone to overfitting --- p-values (and similar values) are \emph{not} valid anymore and effects look too significant.
  \begin{mintlinebox}{R}
    fit.empty <- lm(y~1, data=data)
    fit.full <- lm(y~., data=data)
    fit.bw <- step(fit.full, direction="backward")
    fit.fw <- step(fit.empty, direction="forward", scope=list(upper=fit.full,lower=fit.empty)
  \end{mintlinebox}

\end{sectionbox}

\section{Nonparametric Density Estimation}\label{sec:nonparametric_density_estimation}
\begin{sectionbox}[Kernel estimator]
  Estimate density $\hat f(x) = \frac{1}{nh}\sum_{i=1}^n w((x-X_i)/h)$.
  Kernels include (i) rectangular ($w(x) = 0.5\cdot\mathds{1}_{|x|<1}$), (ii) triangular, or (iii) Gaussian.
  We require $\int_\mathbb{R}K(x)dx = 1$.
  The bandwidth parameter $h$ is crucial and determines the ``smoothness'' of the density estimate.
\end{sectionbox}
\begin{sectionbox}[Choosing a bandwidth $h$]\nospacing{}
  A simple approach is using $k$-nearest neighbors, i.e. $h(x)=\max_{x_i \in \mathit{KNN}_k(x)} ||x-x_i||_2$ with tuning parameter $k$.
  Note that $\int_\mathbb{R} K(x)dx = 1$ might be violated.
  Naturally, the bandwidth also induces a \emph{bias-variance trade-off}.
  Note that
  \(
    MSE(x) = \mathbb{E}\left[\left(\hat f(x)-f(x)\right)^2\right]=\left(\mathbb{E}[\hat f(x)] - f(x)\right)^2 + Var(\hat f(x))
  \), so we can try to minimize the integrated $MSE$ over all points to find the best bandwidth.

  If we know $f$ and $f''$ we can compute the bias and variance analytically (up to some precision).
  From this we can derive global asymptotically optimal bandwidths, which can be useful if we can at least estimate $f$ and $f''$, and this can also be extended to local bandwidths.
\end{sectionbox}
\begin{sectionbox}[Density estimation in higher dimensions]\nospacing{}
  Basically use $\hat f(\vec{x}) = \frac{1}{nh^d}\sum_{i=1}^nK((\vec{x}-\vec{X}_i)/h)$ with a Kernel that supports vectors.
  The Gaussian kernel is the only one that is radially symmetric.
  Note that in higher dimensions, density estimation becomes very hard, due to data points becoming very sparse.
\end{sectionbox}


\section{Nonparametric Regression}\label{sec:nonparametric_regression}
Nonparametric regression with \emph{one} predictor variable, i.e. $Y_{i} = m(x_{i}) + \epsilon_{i}$ with $\epsilon_{1:n}$ i.i.d and $\mathbb{E}[\epsilon_{i}] = 0$. We want $m(x) = \mathbb{E}[Y|x]$ and ``some'' smoothness.

\begin{sectionbox}[Kernel regression estimator]\nospacing{}
  A ``locally weighted'' approach yields the NW kernel estimator
  \begin{equation}\label{eq:nw_regressor}
  \hat m(x) = \frac{\sum_{i=1}^{n} \omega_{i}Y_{i}}{\sum_{i}\omega_{i}} = \argmin_{m_{x} \in \mathbb{R}}\sum_{i=1}^{n}\omega_{i}{(Y_{i}-m_{x})}^{2}
  \end{equation}
  with $\omega_{i} = K\left(\frac{x_{i}-x}{h}\right)$ a kernel centered at $x_{i}$ and bandwidth $h$.
  As $h$ small $\rightarrow$ large then (high variance) $\rightarrow$ (high bias).
  For $x_{i}$ equidistant there exists $h_{\textrm opt} = f(\sigma_{\epsilon}^{2}, m''(x))$ which can be iteratively found.

\begin{mintlinebox}{R}
    ksmooth(x, y, kernel="normal", bandwidth=0.2, x.points=x)$y$
\end{mintlinebox}
%$
\begin{mintlinebox}{R}
  # automatic bandwidth
  fit.lo<-lokerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
  fit.gl<-glkerns(X, Y, x.out=X, hetero=TRUE, is.rand=TRUE)
\end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[Local polynomial regression estimator]\nospacing{}
Instead of finding a local constant $m_{x}$ we can also find a \emph{local polynomial}, i.e.\ we replace $m_{x}$ with $\beta_{1} + \sum_{i=2}^{p} \beta_{i}{(x_{i}-x)}^{i-1}$ (usually $p=2$ or $p=4$).
Often better at edges and yields first derivative.
  \begin{mintlinebox}{R}
  fit.loess <- loess(y ~ x, data=data.frame(x=x, y=y_pert), span=0.2971339, surface='direct')
  fit.loess.pred <- predict(fit.loess, newdata=x)
  \end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[The hat matrix $\mathcal{S}$]\nospacing{}
  We want to construct $\mathcal{S}$ with $\hat{\vec{Y}} = \mathcal{S}\vec{Y}$, i.e.\ the linear operator mapping the labels to the predictions.
  Given the regression (smoothing) function $s$, we compute $\mathcal{S}_{\cdot j} = s(\vec{x}, \vec{e}_{j}, h)$ with $\vec{e}_{j}$ the $j$-th unit vector.
  Then $Cov(\hat m(\vec{x})) = Cov(\mathcal{S} \vec{Y}) = \mathcal{S} Cov(\vec{Y}) \mathcal{S}^{\top} = \sigma_{\epsilon}^{2}\mathcal{S}\mathcal{S}^{\top}$, i.e. $Cov(\hat m(x_{i}), \hat m(x_{j})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S})}^{\top}_{ij}$, and $Var(\hat m(x_{i})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S}^{\top})}_{ii}$.
  Estimate $\hat{\sigma}_{\epsilon}^{2} \approx \sum_{i=1}^{n}{(Y_{i} - \hat m(x_{i}))}^{2}/(n-df)$.
  Then
  \begin{itemize}
    \item $\widehat{s.e.}(\hat m(x_{i})) = \sqrt{\widehat{Var}(\hat m(x_{i}))} = \hat \sigma_{\epsilon} \sqrt{{(\mathcal{S}\mathcal{S}^{\top})}_{ii}}$
    \item $\hat m(x_{i}) \approx \mathcal{N}\left(\mathbb{E}[\hat m(x_{i})], Var(\hat m(x_{i}))\right)$
    \item $I = \hat m(x_{i}) \pm 1.96 \cdot \widehat{s.e.}(\hat m(x_{i}))$ $\rightarrow$ (pointwise) CI
  \end{itemize}

  Additionally we can compute the \emph{degrees of freedom} for regression estimators with $df = \mathbf{tr}(\mathcal{S})$.

  \begin{mintlinebox}{R}
# Construct S matrix
N <- length(x); Eye <- diag(N)
S.nw <- S.lp <- S.ss <- matrix(0, nrow=N, ncol=N)
for (j in 1:N) {
  y_ <- Eye[, j]
  S.nw[, j] <- ksmooth(x, y_, kernel="normal", bandwidth=0.2, x.points=x)$y ;$}

est.nw<-est.lp<-est.ss<-matrix(0,nrow=length(x),ncol=nrep)
for (i in 1:nrep) {
  # generate y with disturbance
  y_pert <- y + rnorm(length(x), mean=0, sd=1)
  # try to fit NW
  est.nw[, i] <- ksmooth(x=x, y=y_pert, kernel="normal", bandwidth=0.2, x.points=x)$y ;$
  sig_sq.nw <- sum((y_pert - est.nw[, i])^2) / (length(y) - sum(diag(S.nw)))
  se.nw[, i] <- sqrt(sig_sq.nw * diag(S.nw \%*\% t(S.nw)))}

  \end{mintlinebox}


\end{sectionbox}
%\subsection{}\label{subsec:smoothing_splines_and_penalized_regression}
\begin{sectionbox}[Smoothing splines and penalized regression]\nospacing{}
High-order polynomials do not work, so splines are used. We discuss splines \emph{without} having to specify the knots.
Find $\argmin_{m \in C^{0}(\mathbb{R})} \sum_{i=1}^{n}(Y_{i} - m(x_{i}))^{2} + \lambda \int_{\mathbb{R}}m''(z)^{2} dz$.
Note that the minimizer is \emph{finite dimensional} --- it is a cubic spline that can be computed using a set of basis functions $m_{\lambda}(x) = \sum_{j=1}^{n}\beta_{j}B_{j}(x)$ or $||\vec{Y} - B\vec{\beta}||^{2}+\lambda \vec{\beta}^{\top}\Omega\vec{\beta} \Rightarrow \hat{\vec{\beta}} = {(B^{\top}B + \lambda\Omega)}^{-1}B^{\top}\vec{Y}$.
Choose $\lambda$ on the scale of $df = \mathbf{tr}(\mathcal{S}_{\lambda})$.
Note that this is Ridge-type regression, which saves us from being overparametrized ($n$ points, $n$ parameters).
In the exam, this is \emph{not} considered ``standard'' least squares.
  \begin{mintlinebox}{R}
  fit.ss<-smooth.spline(x, y_pert, spar=0.6) # attr. cvcrit = loocv
  fit.ss.pred[, i]<-predict(fit.ss, newdata=x)$y ; $
  \end{mintlinebox}
\end{sectionbox}

\section{Cross Validation}\label{sec:cross-validation}
Let $(X_{1}, Y_{1}), \dots, (X_{n}, Y_{n})$ i.i.d $\sim P$.
We would like to compute $\mathbb{E}_{(X_{\textrm new}, Y_{\textrm new})}[\rho(\vec{Y}_{\textrm new}, \hat{m}_{\textrm train} (X_{\textrm new}))]$.
\begin{sectionbox}[Constructing cross-validation datasets]\nospacing{}
  Approaches include
  \begin{description}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item[Validation set:] ---
    \item[Leave-one-out CV:] $n^{-1}\sum_{i=1}^{n}\rho\left(Y_{i}, \hat{m}_{n-1}^{(-i)}(X_{i})\right)$ ca. unbiased.
    \item[$K$-fold CV:] $K^{-1}\sum_{i=1}^{K}|\mathcal{B}_{k}|^{-1}\sum_{i\in\mathcal{B}_{k}}\rho\left(Y_{i}, \hat{m}_{n-|\mathcal{B}_{k}|}^{(-\mathcal{B}_{k})}(X_{i})\right)$. Smaller variance than $1$-CV.
    \item[Random division:] Like $K$-fold, but build $\mathcal{B}_{k}$ by sampling without replacement ($\approx 10\%$). Usually fastest.
  \end{description}
\end{sectionbox}
\begin{sectionbox}[Tricks using hat matrix]\nospacing{}
  For linear fitting operators and the loss $\rho(y, x) = (y-x)^{2}$ we can exploit the hat matrix and get the full $1$CV result in a single step using
  \[
    n^{-1}\sum_{i=1}^{n}\left(Y_{i}-\hat{m}_{n-1}^{(-i)}(X_{i})\right)^{2} = n^{-1}\sum_{i=1}^{n}\left(\frac{Y_{i}-\hat{m}(X_{i})}{1-\mathcal{S}_{ii}}\right)^{2}.
  \]
  It can be cheaper to just compute $\mathbf{tr}(\mathcal{S})$ (instead of all $\mathcal{S}_{ii}$), which leads to the \emph{generalized cross-validation}
  \[
    GCV = \frac{n^{-1}\sum_{i=1}^{n}(Y_{i}-\hat{m}(X_{i}))^{2}}{(1-n^{-1}\mathbf{tr}(\mathcal{S}))^{2}}.
  \]
  The two equations coincide if $\mathcal{S}_{ii}=c\ \forall i$.
\end{sectionbox}

\section{Bootstrap}\label{sec:bootstrap}
\textbf{ROMEO'S PART}
Efron's \emph{parametric} and \emph{nonparametric bootstrap} can be described as ``simulating from an estimated model'' and can be used for \emph{statistical inference (confidence intervals and testing)} and \emph{estimating the predictive power of a model or algorithm}.

\begin{sectionbox}[Nonparametric bootstrap]\nospacing{}
  Let $Z_{1:n}$ i.i.d $\sim P$ with $Z_i=(X_i, Y_i), X_i \in \mathbb{R}^p, Y_i \in \mathbb{R}$, and let $\hat \theta_n = g(Z_{1:n})$ be an estimator.
  We would like to know the \emph{distribution of $\hat \theta_n$}.
  We approximate $\vec{P}$ by the\emph{empirical distribution $\hat{\vec{P}}_n$} that assigns $\mathbb{P}[X_i] = 1/n\ \forall i$.
  Then we can repeatedly sample $Z_{1:n}^\ast$ i.i.d. $\sim \hat P_n$ and compute $\hat\theta_n^\ast = g(Z_{1:n}^\ast)$.
  The histogram (or any density estimator) then describes the distribution of $\hat{\theta}_n^\ast$.
  The algorithm reads
  \begin{enumeratenosep}
    \item Sample (with replacement) $Z_{1:n}^\ast$ i.i.d $\sim \hat P_n$.
    \item Compute the bootstrapped estimator $\hat\theta_n^\ast=g(Z_{1:n}^\ast)$.
    \item Repeat $B$ times to obtain $\hat \theta_n^{\ast 1:B}$.
    \item Approximate $\mathbb{E}^\ast[\hat\theta_n^\ast] \approx B^{-1}\sum_{i=1}^B\hat\theta_n^{\ast i}$ and $Var^\ast(\hat\theta_n^\ast) \approx (B-1)^{-1}\sum_{i=1}^B\left(\hat\theta_n^{\ast i} - B^{-1}\sum_{j=1}^B \hat\theta_n^{\ast j}\right)^2$.
      Then $\alpha$-quantile of $\hat\theta_n^\ast \approx $ empirical $\alpha$-quantile of $\hat\theta_n^{\ast 1:B}$.
  \end{enumeratenosep}
\end{sectionbox}

\begin{notebox}[Central limit theorem]\nospacing{}
  Let $X_i$ be a random variable with $\mathbb{E}[X_i] = 0$ and $Var(X_i) = \sigma^2$. Then $n^{-1}\sum_{i=1}^nX_i \overset{n\to \infty}{\to} \mathcal{N}(\mu, \sigma^2/n)$.
\end{notebox}
\begin{notebox}[Bootstrap consistence]\nospacing{}
  Consistency of the bootstrap typically holds if the limiting distribution of $\hat \theta_n$ is Normal and if $Z_{1:n}$ are i.i.d. 
  Mathematically, for an increasing sequence $a_n$ and $\forall x$, $\mathbb{P}[a_n(\hat\theta_n-\theta)\leq x] - \mathbb{P}^\ast[a_n(\hat\theta_n^\ast - \hat \theta_n) \leq x] \overset{P}\to 0 \text{ as } n \to \infty$.
  Then $Op^\ast(\hat\theta_n^\ast)/Op(\hat\theta_n) \overset{P}{\to} 1$ with $Op \in \{Var, \mathbb{E}\}$.
\end{notebox}

\begin{sectionbox}[Bootstrap confidence interval]\nospacing{}
  Given bootstrap consistence, we can compute $[\hat \theta_n - \hat q_{1-\alpha/2}, \hat\theta_n - \hat q_{\alpha/2}]$ with $\hat q_\alpha = \alpha$-(bootstrap) quantile of $\hat\theta_n^\ast - \hat \theta_n$.
\end{sectionbox}

\textbf{LAUREN'S PART}
\begin{sectionbox}[Nonparametric Bootstrap]
Data $Z = (Z_1,..,Z_n) \sim P$, estimator $\hat{\Theta} = g(Z)$ (e.g. mean). i) Generate Bootstrap sample $Z^{\ast} \sim \hat{P}$ (sampling $n$ points with replacement from $Z$); ii) Compute $\hat{\Theta}^{\ast} = g(Z^{\ast})$; iii) Repeat R times $\Rightarrow \hat{\Theta}^{\ast,1},..,\hat{\Theta}^{\ast,R}$. $\mathbb{E}(\hat{\Theta}^{\ast}) = R^{-1}\sum_{i=1}^{R}\hat{\Theta}^{\ast,i} = \overline{\hat{\Theta}}^{\ast}$. Confidence intervals: 
\begin{enumeratenosep}[label=\roman*]
	\item quantile: $ [q_{\hat{\Theta}^{\ast}}(\alpha/2), q_{\hat{\Theta}^{\ast}}(1-\alpha/2)]  $
	\item rev. quantile: $ [\hat{\Theta} - q_{\hat{\Theta}^{\ast}-\hat{\Theta}}(1-\alpha/2), \hat{\Theta} - q_{\hat{\Theta}^{\ast}-\hat{\Theta}}(\alpha/2)]   $
	\item normal: $2\hat{\Theta}-\overline{\hat{\Theta}}^{\ast} \pm q_X(1-\alpha/2)\cdot \hat{sd}(\hat{\Theta})$ - corrects for bias $\hat{\Theta} - \hat{\Theta}^{\ast}$, $X \sim \mathcal{N}(0,1)$ 
\end{enumeratenosep}
\begin{mintlinebox}{R}
	require("boot")
	tm <- function(x, ind) {mean(x[ind], trim = 0.1)}
	res.boot <- boot(data=sample,statistic=tm,R=10000,
	sim="ordinary")
	# 'basic'=rev.quant., 'norm'=normal, 'perc'=quant.
	boot.ci(res.boot,conf=0.95,type=c("basic","norm","perc"))
	?quantile ?qnorm
\end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[Double bootstrap]
Idea: Find $\alpha'$ s.t. actual coverage of bootstrap CI $I^{\ast}(1-\alpha')$ is equal to $\alpha$.
\begin{enumeratenosep}[label=\roman*]
	\item Draw BS sample $Z^{\ast}$. Sample from $Z^{\ast}$ to obtain $Z^{\ast\ast}$. Compute CI $I^{\ast\ast}(1-\alpha)$ for $\hat{\Theta}^{\ast}$ based on $B$ draws $Z^{\ast\ast}$. Compute coverage of $\hat{\Theta}$ by $I^{\ast\ast}$ (1 or 0).
	\item Repeat i) $M$ times to obtain $M$ coverage values. Compute mean to obtain actual coverage of $I^{\ast\ast}$.
	\item Adjust $\alpha$ and repeat previous steps until $\text{coverage}(I^{\ast\ast}(1-\alpha')) = 1 -\alpha$. Use CI $I^{\ast}(1-\alpha')$
\end{enumeratenosep}

	\begin{mintlinebox}{R}

	\end{mintlinebox}		
\end{sectionbox}	

\begin{sectionbox}[Parametric Bootstrap]
Assume $Z = (Z_1,..,Z_n) ~i.i.d. \sim P_{\Theta}$. Fit $\hat{\Theta} = MLE(Z, P)$ and generate samples $Z^{\ast} ~i.i.d. \sim P_{\hat{\Theta}}$. Usually better than nonparametric version when $P_{\hat{\Theta}}$ is a good fit (e.g. known model structure $P$) and few data points available.
\begin{mintlinebox}{R}
fun.theta <- function(x) {quantile(x, probs = 0.75)}
fun.gen <- function(x,mle) {rgamma(length(x),shape=mle[1],rate=mle[2])}
res.boot <- boot(data,fun.theta,R=1000,sim="parametric",
ran.gen=fun.gen,mle=fit.gamma$estimate); $
\end{mintlinebox}		
\end{sectionbox}	

\begin{sectionbox}[Bootstrap error estimate]
Generalization error (loss $\rho(y,m(x))$) of model $m$ (fitted to full data set) can be estimated by fitting models $m^{\ast,i}$ to bootstrap samples and computing
\begin{itemize}
	\item errors on full data set $e^{\ast,i} = n^{-1}\sum_{i=1}^n \rho(y,m^{\ast,i}(x_i)) $
	\item OOB errors $e^{\ast,i}_{ob} = n_{ob,i}^{-1}\sum_{i=1}^{n_{ob,i}} \rho(y_{ob,i},m^{\ast,i}(x_{ob,i})) $
\end{itemize}
The error of $m$ is then approximated by $R^{-1}\sum_{i=1}^R e^{\ast,i}$.		
\end{sectionbox}

\section{Classification}\label{sec:classification}
Given $(X_1, Y_1), \dots, (X_n,Y_n)$ i.i.d. with $Y_i \in \{0, \dots, J-1\}$, determine $\pi_j(x) = \mathbb{P}[Y=j|X=x]\ \forall j = 0,1,\dots,J-1$.
The optimal classifier is the \emph{Bayes classifier}, which is simply $\mathcal{C}_{\textrm Bayes}(x) = \argmax_{0\leq j\leq J-1}\pi_j(x)$.
Then, the zero-one test set error is called \emph{Bayes risk}, i.e. $\mathbb{P}[\mathcal{C}_{\textrm Bayes}(X_{\textrm new}) \neq Y_{\textrm new}]$.

\begin{sectionbox}[Discriminant analysis]\nospacing{}
  \textbf{Linear case: }Assume $(X | Y=j) \sim \mathcal{N}_p(\mu_j, \Sigma)$, $\mathbb{P}[Y=j] = p_j$, and $\sum_{j=0}^{J-1}p_j=1$.
  Then by Bayes formula $\pi_j(x) = \frac{f_{X|Y=j}(x)\cdot p_j}{\sum_{k=0}^{J-1}f_{X|Y=k}(x)\cdot p_k}$ with each $f_{X|Y=j}$ a Gaussian $\mathcal{N}(\mu_j, \Sigma_{(j)})$.
  We can estimate $\mu_j$ and even $\Sigma$/$\Sigma_j$ using closed formulas, but we also need priors for $Y_i$, which often is picked as $p_j=n_j/n$.
  This results in $\hat \delta_j(x) = (x-\hat{\mu}_j/2)^{\top}\Sigma^{-1}\hat{\mu}_j+\log(\hat p_j)$ with (linear in $x$) decision boundaries $\hat{\delta}_j(x) - \hat \delta_{j'}(x) \geq 0$ and $\mathcal{C}(x) = \argmax_j \hat \delta_j(x)$.

  \textbf{Quadratic case: } Now we assume different $\Sigma_j$ for each class and obtain quadratic decision boundaries $\hat{\delta}_j(x) = -\log(\det(\hat\Sigma_j))/2 - (x-\hat{\mu}_j)^{\top}\hat{\Sigma}_j^{-1}(x-\hat{\mu}_j)/2 + \log(\hat p_j)$.
  The price: $J\cdot p(p+1)/2$ parameters (for all $\Sigma$s) vs. $p(p+1)/2$ for a single $\Sigma$.
\end{sectionbox}

\begin{sectionbox}[Logistic regression for binary classification]\nospacing{}
  Given some model $g: \mathbb{R}^p \to \mathbb{R}$ (e.g. a linear model) we can use the logistic transform $\pi \mapsto \log(\pi/(1-\pi))$ to get probabilities: $\log(\pi(x)/(1-\pi(x))) = g(x)$ and $\pi(x) = 1/(1+\exp{(-g(x))})$.
  This implies $Y_i \sim \text{Bernoulli}(\pi(x_i))$ (e.g. weighted coin flip). The likelihood is $L(\vec{\beta}; (X_i,Y_i)_{i=1:n}) = \prod_{i=1}^n\pi(x_i)^{Y_i}(1-\pi(x_i))^{1-Y_i}$.
  We typically estimate $\vec{\beta}$ using e.g. (Newton's) gradient descent (due to a non-linear problem).
  As $n\to \infty$ we can asymptotically compute the standard errors $\widehat{s.e.}(\hat{\beta}_j)$ and t-test statistics $\hat\beta_j/\widehat{s.e.}(\hat\beta_j) \sim \mathcal{N}(0,1)$ (under $H){0,j}: \beta_j=0)$.
  \begin{mintlinebox}{R}
    fit <- glm(Y~., data=data, family="binomial")
    mean((predict(fit, type="response") > 0.5) == data$Y)$
  \end{mintlinebox}
\end{sectionbox}

\begin{notebox}[Linear predictors]\nospacing{}
  Note that both \emph{LDA} and \emph{Logistic regression} are \emph{linear} in the prediction variables.
  For LDA that comes from the Gaussian assumption (i.e. ``linearization'' of the true distribution), for Logistic regression it comes from the linear log-odds function.
\end{notebox}
\begin{notebox}[Multiclass case ($J>2$)]\nospacing{}
  \begin{enumeratenosep}
  \item $J$ classes $\rightarrow$ $J$ binary variables: $\tilde \pi_j(x) = \frac{\hat pi_j(x)}{\sum_{j=0}^{J-1}\hat\pi_j(x)}$
  \item Using \emph{multinomial distribution} (parametric linear logistic) (see \verb!multinom!)
  \item ``Reference class'' $\log(\pi_j(x)/\pi_0(x)) = g_j(x)$
  \item Pairwise 1-vs-1, fitting ${J \choose 2}\cdot p$ parameters
  \item Exploiting ``ordered'' classes with proportional odds
  \end{enumeratenosep}
\end{notebox}

\textbf{LAUREN's PART}
\begin{sectionbox}[Discriminant Analysis]
Assume the model $(X|Y=j) \sim \mathcal{N}_p(\mu_j, \Sigma), P[Y=j] = p_j, j=0..J-1$. Then, by Bayes:
\[
\mathbb{P}[Y=j|X=x] = \frac{f_{X|Y=j}(x)p_j}{\sum_{k=0}^{J-1} f_{X|Y=k}(x)p_k}
\]
where $f$ is density of respective Gaussian dist. Using estimates:
\begin{enumeratenosep}[label=\roman*]
\item $\hat{\mu}_j = n_j^{-1}\sum_{i;Y_i=j}X_i$, $p_j = n_j/n$ (usually)
\item $\hat{\Sigma} = \frac{1}{n-J}\sum_{j=0}^{J-1}\sum_{i=1}^{n}(X_i-\hat{\mu}_j)(X_i-\hat{\mu}_j)^T \mathbf{1}_{[Y_i=j]}$
\item$\hat{\Sigma}_j = \frac{1}{n_j-1}\sum_{i=1}^{n}(X_i-\hat{\mu}_j)(X_i-\hat{\mu}_j)^T \mathbf{1}_{[Y_i=j]}$
\end{enumeratenosep}
With this, the classifier becomes:
\begin{itemize}
	\item	$\hat{\mathcal{C}}_{LDA} = \argmax_{j} \hat{\delta}_j(x)$
	\item $\hat{\delta}_j(x) = x^T\hat{\Sigma}^{-1}\hat{\mu}_j -\hat{\mu}_j^T\hat{\Sigma}^{-1}\hat{\mu}_j/2 + \log{\hat{p}_j} 
	 =  (x-\hat{\mu}_j/2)^T\hat{\Sigma}^{-1}\hat{\mu}_j + \log{\hat{p}_j}$
\end{itemize}
For QDA, we have $\hat{\delta}_j(x) =  - \log{(\det{(\hat{\Sigma}_j)})}/2 +  (x-\hat{\mu}_j)^T\hat{\Sigma}_j^{-1} (x-\hat{\mu}_j)/2 + \log{\hat{p}_j} $
\begin{mintlinebox}{R}
?lda ?qda
\end{mintlinebox}	
\end{sectionbox}

\begin{sectionbox}[Logistic Regression]
Binary classification $Y \in \{0,1\}$, $\pi(x) = \mathcal{P}[Y=1|X]$. Linear logistic model $\log(\frac{\pi(x)}{1- \pi(x)}) = g(x) = \sum_{i=1}^p \beta_i x_i$. We have $Y_i \sim \text{Bernoulli}(\pi(x))$. Hence, the likelihood is given by $L(\beta;(x_1,Y_1),..,(x_n,Y_n))$ = $\prod_{i=1}^{n}\pi(x_i)^{Y_i}(1-\pi(x_i))^{1-Y_i}$. Neg. log likelihood:
\begin{align*}
	l &= -\sum_{i=1}^{n}[ Y_i \log{(\pi(x_i))} + (1-Y_i)(1- \log{(\pi(x_i))})] \\
	&=  -\sum_{i=1}^{n}[Y_i g(x) - \log{(\exp{(g(x))} + 1)}]
\end{align*}
%\[l = -\sum_{i=1}^{n}[ Y_i \log{(\pi(x_i))} + (1-Y_i)(1- \log{(\pi(x_i))})] =  %-\sum_{i=1}^{n}[Y_i g(x) - \log{(\exp{(g(x))} + 1)}]
%\]
MLE thus yields a nonlinear optimization problem. For multiclass, define $J$ separate binary problems, yielding $\pi_j$  and use $\hat{\mathcal{C}}_{LG} = \argmax_{j} \frac{\hat{\pi}_j(x)}{\sum_{j=0}^J \hat{\pi}_j(x)}$. Alternatively, use \texttt{multinom}, multinomial distribution model with $L=\prod_{i=0}^{J-1}\pi_i^{n_i}$ and $l=\sum_{i=0}^{J-1}n_i\log{(\pi_i)}$.
\begin{mintlinebox}{R}
fit = glm(formula=y ~ .,family="binomial",data=data)
# performance analysis
require(ROCR) ?prediction ?performance
\end{mintlinebox}	
\end{sectionbox}


\textbf{ROMEO's PART}
\section{Flexible regression and classification methods}\label{sec:flexible_regression_and_classification_methods}
We fight the \emph{curse of dimensionality} by making some structural assumptions (although staying with methods $g(\cdot): \mathbb{R}^p \to \mathbb{R}$ of nonparametric nature).
\subsection{Additive models}%
\label{sub:additive_models}
\begin{sectionbox}\nospacing{}
  Decompose multivariate function in bias plus sum of univariate functions, i.e. $g_{\textrm add}: \mathbb{R}^p\to\mathbb{R}, x\mapsto g_{\textrm add}(x) = \mu + \sum_{j=1}^pg_j(x_j)$ with $g_j(\cdot): \mathbb{R}\to\mathbb{R}, \mathbb{E}[g_j(X_j)]=0$.
  Note that the zero-mean requirement for each $g_j(\cdot)$ makes the problem well posed.
  This approach is a generalization of linear models, and similarly can not model interaction terms $g_{j,k}(x_j, x_k)$.
  Due to the way they are constructed, additive linear models \emph{avoid the curse of dimensionality}!
\end{sectionbox}
\begin{sectionbox}\nospacing{}
  To construct the models, let $\mathcal{S}_j$ be a smoothing technique (e.g. \emph{Nadaraya-Watson Gaussian kernel estimators}).
  Then, the \textbf{backfitting} algorithm works as follows:
  \begin{itemizenosep}
    \item Compute $\hat \mu = n^{-1}\sum_{i=1}^n Y_i$ and initialize $\hat g_j(\cdot) \coloneqq 0$.
    \item Cycle through the indices $j = 1,2,\dots,p,1,2,\dots,p,1,2,\dots$ and update
      $\hat g_j=\mathcal{S}_j(\vec{Y}-\hat \mu\vec{1}-\sum_{k\neq j}\hat g_k)$.
      Stop each function at convergence.
    \item Normalize the functions: $\tilde g_j(\cdot) = \hat g_j(\cdot) - n^{-1}]sum_{i=1}^n\hat g_j(X_{ij})$.
  \end{itemizenosep}
  This basically makes the algorithm repeatedly solve the 1-dimensional fitting problem.
  The algorithm may be slow but often works and can use any 1-dimensional fitting technique.
\end{sectionbox}
\begin{sectionbox}\nospacing{}
  When fitting Additive models in R with the function \verb!gam!, the smoothers $\mathcal{S}_j$ penalized regression spline, and the degrees of freedom for each spline (i.e. each variable) will be determined through cross-validation.

  \begin{mintlinebox}{R}
    fit <- gam(Y ~ s(x1) + s(x2) + ..., data=data)
    plot(fit, pages=1, shade=TRUE)
    sfsmisc::TA.plot(fit, labels="o")
  \end{mintlinebox}
\end{sectionbox}

\textbf{LAUREN's PART}
\begin{sectionbox}[Additive Models]
$g_{add}(\mathbf{x}) = \mu + \sum_{j=1}^{p} g_j(x_j) : \mathbb{R}^p \rightarrow \mathbb{R}$, $\mathbb{E}[g_j(X_j)] = 0$. Nonparametric smoothers (splines, NW, etc.) $\mathcal{S}_j$ can be used on each predictor $x_j$. Smoothers can be found using backfitting:
\begin{enumeratenosep}[label=\roman*]
\item Set $\hat{\mu} = \overline{Y}, \hat{g}_j(\cdot) = 0$
\item Cycle $j=1..p$: $\mathbf{\hat{g}_j} = \boldsymbol{\mathcal{S}}_j(\mathbf{Y}- \hat{\mu} -\sum_{k\neq j}\mathbf{\hat{g}}_k) \in \mathbb{R}^n $ until $\hat{g}_j$ do not change significantly (rel. tol $10^{-6}$).
\item Normalize $\tilde{g}_j(\cdot) = \hat{g}_j(\cdot) - n^{-1}\sum_{i=1}^{n}\hat{g}_j(X_{ij})$	
\end{enumeratenosep}
\texttt{gam} fits penalized regression splines as smoothers. A similar, simple option is to perform linear regression with added polynomial (or any) effects. This requires explicit selcetion of effects (see variable selection), whereas \texttt{gam} selects the complexity of $\mathcal{S}_j$ automatically.
\begin{mintlinebox}{R}
library(mgcv)
fit <- gam(y ~ s(x1)+ s(x2) + s(x3), data=data)
form1 <- as.formula("y~.") #poly. LS of deg. 2 in all xj
form2 <- wrapFormula(form1,
	data=data,wrapString="poly(*,degree=2)")
fit2 <- lm(form2, data=data)
\end{mintlinebox}
\end{sectionbox}

\subsection{Multivariate adaptive regression splines}%
\label{sub:multivariate_adaptive_regression_splines}
\begin{sectionbox}[MARS]\nospacing{}
$g(\mathbf{x}) = \mu + \sum_{m=1}^{M}\beta_m h_m(\mathbf{x}) = \sum_{m=0}^{M}\beta_m h_m $
Find $h \in \mathcal{M}$ functions by foward selection and pruning:

\begin{enumeratenosep}[label=\roman*]
	\item Initialize $\mathcal{M} = \{ h_0 = 1\}, \beta_0 = \overline{Y}$
	\item For $r=1,2,..$: Find best pair (most reduction of RSS) $h_{2r-1}=h_l(\cdot)\times(x_j-x_{i,j})_{+}, h_{2r}=h_l(\cdot)\times(x_{i,j}-x_j)_{+}$, where $h_l \in \mathcal{M}$ does not already depend on $x_j$. Estimate $\beta_{2r-1}, \beta_{2r}$ by LS. Add $h_{2r-1}, h_{2r}$ to $\mathcal{M}$.
	\item Repeat until $\mathcal{M}$ large enough. Prune by repeatedly removing one function from pairs $h_{2r-1}, h_{2r}$ (least increase in RSS). Stop when GCV score is optimized.
$(x_j-d)_{+} = x_j-d$, if $x_j \geq d$, zero otherwise.
\end{enumeratenosep}
\begin{mintlinebox}{R}
require("earth"); 
fit <- earth(formula=y~.,data=data, degree=2)
plotmo(fit, degree2=FALSE, caption="main effects")
\end{mintlinebox}
\end{sectionbox}	

\begin{sectionbox}[Neural networks]\nospacing{}
	$g(\mathbf{x})_k = f_0(\alpha_k + \sum_{h=1}^{q}w_{hk}\sigma(\tilde{\alpha_h} + \sum_{j=1}^{p}\tilde{w}_{jh}x_j)) \forall k=1..J$, $q$ hidden nodes, $J$ output dim. Activation $\sigma(t) = \frac{\exp t}{1 + \exp t}$. For regression, $f_0$ identity, for classification $f_0 = \sigma$ and $\mathcal{C}_{NN} = \argmax_{j} g_j(\mathbf{x})$. Many other architectures possible, e.g. including a component directly connecting input to output by linear regression.
	\begin{mintlinebox}{R}
	library(nnet); ?nnet ?ppr 
	\end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[Classification and Regression Trees]\nospacing{}
$g_{tree}(\mathbf{x}) = \sum_{r=1}^{M}\beta_r 1_{[\mathbf{x}\in \mathcal{R}_r]}$, where $\mathcal{P} \{\mathcal{R}_1,.., \mathcal{R}_M\}$ is a partition of $\mathbb{R}^p$. The function is piecewise constant. When partition is given, estimate $\hat{\beta}_r = \sum_{i=1}^{n}Y_i 1_{[\mathbf{x}\in \mathcal{R}_r]}/\sum_{i=1}^{n}1_{[\mathbf{x}\in \mathcal{R}_r]}$. For multiclass classification $\hat{\pi}_j(\mathbf{x}) = \sum_{i=1}^{n}1_{[Y_i =j]} 1_{[\mathbf{x}\in \mathcal{R}_r]}/\sum_{i=1}^{n}1_{[\mathbf{x}\in \mathcal{R}_r]}$ for $\mathbf{x} \in \mathcal{R}_r$. Greddy algorithm to find axes parallel partition:
\begin{enumeratenosep}[label=\roman*]
	\item Initialize $M=1$ subset $\mathcal{P} = \{\mathcal{R} = \mathbb{R}^p\}$
	\item Split $\mathcal{R}$ at $d$ in dimension $j$, where $d$ is from the set of midpoints of observed values. Select $j,k$ s.t. neg. log-likelihood decrease is maximized by refinement.
	\item Apply ii) to one cell of the current partition (select like above). Add the resulting two cells and remove the refined one. 
	\item Iterate iii) until until specified max. partition size is achieved
	\item Prune tree by removing leaves resulting in smalles increase in some (CV) metric.
\end{enumeratenosep}
We define the size of a tree $\mathcal{T}$ as the number of leaves (1 + +cuts). For some goodness-of-fit measure $\mathcal{R}(\mathcal{T})$ (e.g. SSE, NLL), the cost-complexity measure is $\mathcal{R}_{\alpha}(\mathcal{T})=\mathcal{R}(\mathcal{T}) + \alpha \text{size}(\mathcal{T}) $. For some $\alpha$, we thus choose $\mathcal{T}(\alpha) = \argmin_{\mathcal{T}\subset\mathcal{T}_M}$. $\alpha$ is then chosen by CV. 1 s.e. rule: Choose smalles tree s.t. its performance is at most one standard error larger than the minimal one. 

\begin{mintlinebox}{R}
library(rpart); require(rpart.plot); 
# cp = alpha
rp <- rpart(y~.,data = data,control=rpart.control(cp=0.0, minsplit=30))
plotcp(rp); cps = printcp(tree)
nid <- 10 # select from plot by 1se rule
cp.opt <- cps[which(cps[,'nsplit']==nid), 'CP']
pruned.tree = prune.rpart(tree, cp=cp.opt)
\end{mintlinebox}
\end{sectionbox}


\subsection{Trees}%
\label{sub:trees}

\subsection{Ridge and Lasso}%
\label{sub:ridge_and_lasso}


\section{Bagging and Boosting}\label{sec:bagging_and_boosting}
\begin{sectionbox}[Bagging and Subbagging]\nospacing{}
  \textbf{B}ootstrap \textbf{agg}regat\textbf{ing} (bagging) (mostly on trees), uses $\hat g(\cdot): \mathbb{R}^{p}\to \mathbb{R}$ and ensembles them (which comes at the loss of interpretability).
  \begin{enumeratenosep}[label=\roman*]
    \item Generate bootstrap sample $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{n}^{\ast}, Y_{n}^{\ast})$ and compute $\hat{g}^{\ast}_{i=1}(\cdot)$. Repeat $B$ times.
    \item Aggregate bootstrap estimates with $\hat{g}_{\textrm Bag}(\cdot) = B^{-1}\sum_{i=1}^{B} \hat{g}^{\ast}_{i}(\cdot) \approx \mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)]$.
  \end{enumeratenosep}
  Note that $\hat{g}_{\textrm Bag}(\cdot) = \hat{g}(\cdot) + \underbrace{(\mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)] -\hat{g}(\cdot))}_{\text{bootstrap bias estimate}}$.
  We can reduce variance at price of higher bias (at least for trees).
  In fact, for many $x$, $Var(\hat{g}_{\textrm Bag}(x)) < Var(\hat{g}(x))$. We can use larger trees (higher variance) to balance the bias-variance trade-off.

  For \textbf{Sub}sample \textbf{agg}regat\textbf{ing} (Subbagging), we draw $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{m}^{\ast}, Y_{m}^{\ast})$ without replacement (e.g. with $m = \lfloor n/2\rfloor$), which can be cheaper overall and is equivalent to Bagging in some simple settings.
\end{sectionbox}

\begin{sectionbox}[$L_{2}$Boosting]\nospacing{}
  Similar to Bagging, iterates on a ``base-learner'' by continually adding a fit on the residuals.
  \begin{enumeratenosep}[label=\roman*]
    \item Get first fit $\hat{g}_{1}(\cdot)$ by fitting on the full data. Compute residuals $U_{i} = Y_{i} - \hat{g}_{1}(X_{i})$ and let $\hat{f}_{1}(\cdot) = \nu \hat{g}_{1}(\cdot)$ with $0 < \nu \leq 1$ (typically $\nu = 0.1$).
    \item For $m = 2, 3, \dots, M$ fit $\hat{g}_{m}(\cdot)$ on residuals $U_{i}$ and set $\hat{f}_{m}(\cdot) = \hat{f}_{m-1}(\cdot) + \nu \hat{g}_{m}(\cdot)$ (and update residuals using $\hat{f}_{m}(\cdot)$).
  \end{enumeratenosep}
  The main tuning parameter is the stopping point $M$. Boosting \emph{increases} the bias and can be used to ensemble trees to fit more complex data. See e.g.
  \begin{mintlinebox}{R}
      ?mboost; ?xgboost; ?gbm;
  \end{mintlinebox}
\end{sectionbox}

\section{Multiple Linear Regression}\label{sec:multiple_linear_regression}
\subsection{The Linear Model}\label{subsec:the_linear_model}
\begin{sectionbox}\nospacing{}
Assume $Y_i = x_{i}^{\top}\beta + \epsilon_{i}$ or $Y = X \times \beta + \epsilon$ with $X \in \mathcal{R}^{(n \times p)}; (n > p)$ and $\mathbb{E}[\epsilon_{i}]=0, Var(\epsilon_{i}) = \sigma^{2}$.
$X$ is often augmented with $(1_{N\times 1})$ to use $\beta_{1}$ as bias.
\end{sectionbox}
\subsection{Least Squares Method}\label{subsec:least_squares_method}
\begin{sectionbox}\nospacing{}
  Least squares estimator is $\hat \beta = \argmin_{\beta} ||Y - X\beta||^{2}_{2} = {(X^{\top}X)}^{-1} X^{\top}Y$ (orthogonal projection of $Y$ onto $span(X)$).
  Estimate $\hat{\sigma}^{2} = \frac{1}{n-p}\sum_{i=1}^{n}r_{i}^{2}$ with $\mathbb{E}[\hat{\sigma}^{2}] = \sigma^{2}$.
\end{sectionbox}

\begin{notebox}[Assumptions for Linear Model]\nospacing{}
    \begin{enumeratenosep}[label=\roman*]
        \item Linear regression equation is correct, i.e. $\mathbb{E}[\epsilon_{i}]=0\ \forall i$.
        \item We measure $x_{i}$'s exactly. Else, need correction (?).
        \item Error is homoscedastic, i.e. $Var(\epsilon_{i})=\sigma^{2}\ \forall i$. Else, use ``Weighted LS''.
        \item Errors are uncorrelated, i.e. $Cov(\epsilon_{i}, \epsilon_{j}) = 0\ \forall i \neq j$. Else ``Generalized LS''.
        \item Errors are jointly normally distributed. Else ``Robust Methods''.
    \end{enumeratenosep}
\end{notebox}

\begin{notebox}[Moments of least squares estimates]\nospacing{}
  Assume $\vec{Y}=X\vec{\beta} + \epsilon, \mathbb{E}[\vec{\epsilon}]=\vec{0},\ Cov(\vec{\epsilon} \vec{\epsilon}^{\top}) = \sigma^{2}I$ (all assumptions satisfied). Then
  \begin{enumeratenosep}[label=\roman*]
    \item $\mathbb{E}[\hat{\vec{\beta}}] = \vec{\beta}$ ($\hat{\vec{\beta}}$ is unbiased).
    \item $\mathbb{E}[\hat{\vec{Y}}] = \mathbb{E}[\vec{Y}] = X\vec{\beta}$ and $\mathbb{E}[\vec{r}] = \vec{0}$.
    \item $Cov(\hat{\vec{\beta}}) = \sigma^{2}{(X^{\top}X)}^{-1}$.
    \item $Cov(\hat{\vec{Y}}) = \sigma^{2} P, Cov(\vec{r}) = \sigma^{2}(I-P)$.
  \end{enumeratenosep}
  If additionally $\epsilon_{i}, \dots, \epsilon_{n} \text{ i.i.d. } \sim \mathcal{N}(0, \sigma^{2})$, then
  \begin{enumeratenosep}[label=\roman*]
    \item $\hat{\vec{\beta}} \sim \mathcal{N}_{p}{(\vec{\beta}, \sigma^{2}(X^{\top}X))}^{-1}$
    \item $\hat{\vec{Y}} \sim \mathcal{N}_{n}(X\vec{\beta}, \sigma^{2}),\ \vec{r} \sim \mathbb{N}_{n}(\vec{0}, \sigma^{2}(I-P))$
    \item $\hat \sigma^{2} \sim \frac{\sigma^{2}}{n-p}\chi^{2}_{n-p}$.
  \end{enumeratenosep}
  Even when normality assumption doesn't hold, central limit theorem is a justification.
\end{notebox}
\subsection{Tests and Confidence Regions}\label{subsec:tests_and_confidence_regions}
\begin{sectionbox}[T-test]\nospacing{}
  Assume linear model with Gaussian errors (or ``large enough'' sample size), s.t. $\hat{\vec{\beta}} \sim \mathcal{N}_{p}\left(\vec{\beta}, \sigma^{2}{(X^{\top}X)}^{-1}\right)$ is normally distributed.
  Then we can test the null-hypothesis $H_{0,j}: \beta_{j = 0}$ against $H_{A,j}: \beta_{j \neq 0}$:
  \[\frac{\hat \beta_{j}}{\sqrt{\sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim \mathcal{N}(0,1)\] under the null-hypothesis $H_{0,j}$. Since $\sigma^{2}$ is unknown, replace by $\hat \sigma^{2} \rightarrow$ T-test:
  \[T_{j} = \frac{\hat \beta_{j}}{\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}}} \sim t_{n-p}\] under the null-hypothesis $H_{0,j}$. Note that $t_{n-p} \approx \mathcal{N}$.
\end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 As individual t-test for $H_{0,j}$ gives the effect of $\beta_{j}$ after subtracting the linear effect of all $\beta_{i\neq j}$.
\end{attentionbox}

 \begin{sectionbox}[Global null hypothesis and ANOVA]\nospacing{}
  We can also check the global null-hypothesis $H_{0}: \beta_{2} = \cdots = \beta_{p} = 0$ using the \emph{analysis of variance} (ANOVA), which decomposes
  \[||\vec{Y} - \bar{\vec{Y}}||^{2}_{2} = ||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}_{2} + ||\vec{Y} - \hat{\vec{Y}}||^{2}_{2}.\]
  Under the global null-hypothesis $\mathbb{E}[\vec{Y}] = \mathcal{E}[\bar{\vec{Y}}] = const$. (no effect of predictor variables). Basically dividing $\sigma^{2}$ by $\hat \sigma^{2}$ yields F-statistic:
  \[F = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}/(p-1)}{||\vec{Y} - \hat{\vec{Y}}||^{2} / (n-p)} \sim F_{p-1,n-p}\] under the global null-hypothesis $H_{0}$.
  ANOVA also yields \emph{goodness of fit} $R^{2} = \frac{||\hat{\vec{Y}} - \bar{\vec{Y}}||^{2}}{||\vec{Y} - \bar{\vec{Y}}||^{2}}$, which should be around $1$.
  Finally, we can also build a confidence interval using $\hat \beta_{j} \pm \sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} \cdot t_{n-p;1-\alpha/2}$.
 \end{sectionbox}

\begin{attentionbox}{Attention}\nospacing{}
 In \verb!summary.lm!, the term \emph{Std. Error} is $\sqrt{\hat \sigma^{2}{{(X^{\top}X)}^{-1}_{jj}}} = \sqrt{\hat{Var}(\hat \beta_{j})}$.
\end{attentionbox}

\subsection{Checking Model Assumptions}\label{subsec:checking_model_assumptions}
\begin{sectionbox}[Tukey-Anscome Plot]\nospacing{}
  Error should fluctuate randomly. If error increases linearly, do log-transform $\vec{Y} \mapsto \log{\vec{Y}}$. If error increases with $\sqrt{Y}$, do a square-root-transform $\vec{Y} \mapsto \sqrt{\vec{Y}}$.
\end{sectionbox}
\begin{sectionbox}[QQ-Plot/Normal-Plot]\nospacing{}
  Plot empirical quantiles of residuals on y versus the theoretical quantiles of $\mathcal{N}(0,1)$ on x.
  If assumption holds, get straight line with intercept $\mu$ and slope $\sigma$.
  Z-shape: long-tailed distr.; Curved: skewed distr.
\end{sectionbox}

\subsection{Model Selection}\label{subsec:model_selection}
\begin{sectionbox}\nospacing{}
  Assume again $\mathbb{E}[\epsilon_{i}] = 0, Var(\epsilon_{i} = \sigma^{2})$.
  We need to adress \emph{bias-variance-tradeoff}.
  Bias is defined as $\mathbb{E} [\hat f(x)] - f(x)$, variance as $q/n \cdot \sigma^{2}$ with $q \leq p$.
\end{sectionbox}
\begin{sectionbox}[Mallows $C_{p}$ statistic]\nospacing{}
  Let $SSE(\mathcal{M})$ the residual sum of squares.
  Then $n^{-1} \sum_{i=1}^{n} \mathbb{E}\left[{(f(x) - \hat f_{\mathcal{I}}(x))}^{2}\right] \approx n^{-1}SSE(\mathcal{M})-\hat \sigma^{2} + 2\hat\sigma^{2}|\mathcal{M}|/n$, with $\mathcal{I}$ the indices of selected predictors and $|\mathcal{I}| = q$.
  Thus, we search for the model that minimizes the $C_{p}$-statistic with $C_{p}(\mathcal{M}) = \frac{SSE(\mathcal{M})}{\hat \sigma^{2}} - n + 2|\mathcal{M}|$.
  Otherwise Akaike's information criterion (AIC) or Bayesian information criterion (BIC). AIC is equivalent to $C_{p}$ for linear Gaussian models.
\end{sectionbox}
\begin{sectionbox}[Forwards and backwards selection]\nospacing{}
  \begin{description}[topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
    \item[Forward selection] (i) Start with empty model. (ii) (Greedily) Keep adding variable that reduces the residual sum of squares. (iii) When done, pick submodel which minimizes $C_{p}$.
    \item[Backward selection] (i) Start with full model. (ii) (Greedily) Keep excluding predictor that increases the residual sum of squares the least. (iii) When done, pick submodel which minimizes $C_{p}$.
  \end{description}
  Backwards selection typically better but more expensive. When $p \geq n$, use forward selection.
  Both methods prone to overfitting --- p-values (and similar values) are \emph{not} valid anymore and effects look too significant.

\end{sectionbox}


\section{Nonparametric Regression}\label{sec:nonparametric_regression}
Nonparametric regression with \emph{one} predictor variable, i.e. $Y_{i} = m(x_{i}) + \epsilon_{i}$ with $\epsilon_{1:n}$ i.i.d and $\mathbb{E}[\epsilon_{i}] = 0$. We want $m(x) = \mathbb{E}[Y|x]$ and ``some'' smoothness.

\subsection{The Kernel Regression Estimator}\label{subsec:kernel_regression_estimator}
\begin{sectionbox}[Nadaraya-Watson kernel estimator]\nospacing{}
  A ``locally weighted'' approach yields the NW kernel estimator
  \begin{equation}\label{eq:nw_regressor}
  \hat m(x) = \frac{\sum_{i=1}^{n} \omega_{i}Y_{i}}{\sum_{i}\omega_{i}} = \argmin_{m_{x} \in \mathbb{R}}\sum_{i=1}^{n}\omega_{i}{(Y_{i}-m_{x})}^{2}
  \end{equation}
  with $\omega_{i} = K\left(\frac{x_{i}-x}{h}\right)$ a kernel centered at $x_{i}$ and bandwidth $h$.
  As $h$ small $\rightarrow$ large then (high variance) $\rightarrow$ (high bias).
  For $x_{i}$ equidistant there exists $h_{\textrm opt} = f(\sigma_{\epsilon}^{2}, m''(x))$ which can be iteratively found.

\begin{mintlinebox}{R}
    library (lokern); fit.lo <- lokerns (x, y)
\end{mintlinebox}

Instead of finding a local constant $m_{x}$ we can also find a local polynomial, i.e.\ we replace $m_{x}$ with $\beta_{1} + \sum_{i=2}^{p} \beta_{i}{(x_{i}-x)}^{i-1}$ (usually $p=2$ or $p=4$).
Often better at edges and yields first derivative.
\end{sectionbox}
\begin{sectionbox}[The hat matrix $\mathcal{S}$]\nospacing{}
  We want to construct $\mathcal{S}$ with $\hat{\vec{Y}} = \mathcal{S}\vec{Y}$, i.e.\ the linear operator mapping the labels to the predictions.
  Given the regression (smoothing) function $s$, we compute $\mathcal{S}_{\cdot j} = s(\vec{x}, \vec{e}_{j}, h)$ with $\vec{e}_{j}$ the $j$-th unit vector.
  Then $Cov(\hat m(\vec{x})) = Cov(\mathcal{S} \vec{Y}) = \mathcal{S} Cov(\vec{Y}) \mathcal{S}^{\top} = \sigma_{\epsilon}^{2}\mathcal{S}\mathcal{S}^{\top}$, i.e. $Cov(\hat m(x_{i}), \hat m(x_{j})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S})}^{\top}_{ij}$, and $Var(\hat m(x_{i})) = \sigma_{\epsilon}^{2}{(\mathcal{S}\mathcal{S}^{\top})}_{ii}$.
  We can estimate $\sigma_{\epsilon}^{2} \approx \sum_{i=1}^{n}{(Y_{i} - \hat m(x_{i}))}^{2}/(n-df)$.
  Then
  \begin{itemize}
    \item $\widehat{s.e.}(\hat m(x_{i})) = \sqrt{\widehat{Var}(\hat m(x_{i}))} = \hat \sigma_{\epsilon} \sqrt{{(\mathcal{S}\mathcal{S}^{\top})}_{ii}}$
    \item $\hat m(x_{i}) \approx \mathcal{N}\left(\mathbb{E}[\hat m(x_{i})], Var(\hat m(x_{i}))\right)$
    \item $I = \hat m(x_{i}) \pm 1.96 \cdot \widehat{s.e.}(\hat m(x_{i}))$ $\rightarrow$ (pointwise) CI
  \end{itemize}

  Additionally we can compute the \emph{degrees of freedom} for regression estimators with $df = \mathbf{tr}(\mathcal{S})$.

\end{sectionbox}
\subsection{Smoothing splines and penalized regression}\label{subsec:smoothing_splines_and_penalized_regression}
\begin{sectionbox}\nospacing{}
High-order polynomials do not work, so splines are used. We discuss splines \emph{without} having to specify the knots.
Find $\argmin_{m \in C^{0}(\mathbb{R})} \sum_{i=1}^{n}(Y_{i} - m(x_{i}))^{2} + \lambda \int_{\mathbb{R}}m''(z)^{2} dz$.
Note that the minimizer is \emph{finite dimensional} --- it is a cubic spline that can be computed using a set of basis functions $m_{\lambda}(x) = \sum_{j=1}^{n}\beta_{j}B_{j}(x)$ or $||\vec{Y} - B\vec{\beta}||^{2}+\lambda \vec{\beta}^{\top}\Omega\vec{\beta} \Rightarrow \hat{\vec{\beta}} = {(B^{\top}B + \lambda\Omega)}^{-1}B^{\top}\vec{Y}$.
Choose $\lambda$ on the scale of $df = \mathbf{tr}(\mathcal{S}_{\lambda})$.
Note that this is Ridge-type regression, which saves us from being overparametrized ($n$ points, $n$ parameters).
In the exam, this is \emph{not} considered ``standard'' least squares.
\end{sectionbox}

\section{Cross Validation}\label{sec:cross-validation}

\section{Bootstrap}\label{sec:bootstrap}

\section{Classification}\label{sec:classification}

\section{Flexible regression and classification methods}\label{sec:flexible_regression_and_classification_methods}
